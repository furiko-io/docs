{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"guide/components/","text":"Components Furiko is organized into the following components, which groups together related functionality and also serves as individual deployment units. More information about the architecture can be found here . Execution Execution is the heart of Furiko, consists of components to manage, execute and configure jobs and workflows. The following Custom Resource Definitions (CRDs) are introduced: JobConfig Job Read more at Execution Concepts . Federation Federation is an optional add-on to manage Furiko jobs across multiple Kubernetes clusters, performing cluster-level drain and cordon operations to federate jobs between clusters safely. Coming Soon! The Federation component is currently only available in our closed-source project, and will be eventually ported to Furiko. Telemetry Telemetry is an optional add-on consisting of components for notifications, monitoring and telemetry data, extending the Kubernetes control plane to provide richer insights into executions running on multiple clusters. Coming Soon! The Telemetry component is currently only available in our closed-source project, and will be eventually ported to Furiko.","title":"Components"},{"location":"guide/components/#components","text":"Furiko is organized into the following components, which groups together related functionality and also serves as individual deployment units. More information about the architecture can be found here .","title":"Components"},{"location":"guide/components/#execution","text":"Execution is the heart of Furiko, consists of components to manage, execute and configure jobs and workflows. The following Custom Resource Definitions (CRDs) are introduced: JobConfig Job Read more at Execution Concepts .","title":"Execution"},{"location":"guide/components/#federation","text":"Federation is an optional add-on to manage Furiko jobs across multiple Kubernetes clusters, performing cluster-level drain and cordon operations to federate jobs between clusters safely. Coming Soon! The Federation component is currently only available in our closed-source project, and will be eventually ported to Furiko.","title":"Federation"},{"location":"guide/components/#telemetry","text":"Telemetry is an optional add-on consisting of components for notifications, monitoring and telemetry data, extending the Kubernetes control plane to provide richer insights into executions running on multiple clusters. Coming Soon! The Telemetry component is currently only available in our closed-source project, and will be eventually ported to Furiko.","title":"Telemetry"},{"location":"guide/features/","text":"Features Furiko specializes in time-based scheduling of scheduled and adhoc jobs. The following is a non-exhaustive list of features and enhancements offered by Furiko. Feature List Timezone-aware Cron Scheduling Furiko allows users to specify a cron schedule to automatically schedule jobs, supporting up to a per-second granularity. Furiko also offers native support for scheduling jobs on a cron schedule with timezones . This allows users to specify their cron schedule in a timezone that is familiar to them, or one that is standardized in their application. If not specified, Furiko also supports specifying a cluster-wide default timezone for all cron schedules. Cluster-wide Load Balancing Furiko offers a unique, extended cron syntax that may drastically improve the performance of running distributed cron at scale. For example, specifying H/5 * * * * means to run every 5 minutes, on a \"random\" minute/second within the 5 minute range. This helps to avoid thundering herd effects in clusters when thousands of other jobs also start at the exact same time, evenly spreading out job executions which reduces waiting duration and impact on other downstream dependencies. Strong Concurrency Handling Furiko introduces stronger handling of multiple concurrent jobs . Using the Forbid concurrency policy, Furiko takes a very strict approach to ensuring that multiple jobs will never be started at the same time, free of race conditions. In addition, adhoc jobs executed will be subject to the same concurrency policy as if it were scheduled automatically, which may help prevent untimely incidents. Scheduling Future Executions Furiko allows ad-hoc jobs to be queued for execution at a later time , or to start a new job execution immediately once the current job execution is finished. Preventing Missed and Double Executions Furiko provides strong guarantees to schedule jobs even in the face of failure, and is able to prevent both double runs and missed runs in many cases. Furiko prevents double scheduled runs by using deterministic name formats . It also prevents missed runs using back-scheduling to tolerate short downtime, allowing the cluster administrator to safely restart or upgrade Furiko at any time. Enhanced Timeout Handling Furiko offers additional timeouts that can be used during a job execution. For example, pending timeouts help to gracefully handle node outages instead of hanging on a single execution, ensuring forward progress for job configs that are automatically scheduled. Job Options Furiko allows you to use JobConfigs as templates for Jobs, by passing parameters into the Job. The JobConfig can be parameterized with Job Options , which defines a structured input that will be validated and substituted at runtime. Running multiple parallel tasks per Job Furiko supports parallelism of tasks within a Job, which supports index number-based, index key-based or matrix-based expansion of tasks that will be run in parallel. A completion strategy can also be defined that will automatically terminate all parallel tasks when certain conditions are met. Federation Across Multiple Clusters Furiko also provides add-on support for federating, cordoning and draining of JobConfigs across multiple Kubernetes clusters safely. Info This feature is currently planned in the Roadmap . Monitoring, Notifications and Telemetry Furiko also provides add-on support for monitoring and notifications of job executions and failures, via a variety of methods including Prometheus metrics, Slack webhooks and email notifications. In addition, Furiko provides alternative API servers to query, store and analyze large amounts of job executions per day, reducing the stress on kube-apiserver . Info This feature is currently planned in the Roadmap . Comparison with batch/v1 The following is a short comparison between Furiko and batch/v1 Jobs and CronJobs : Feature Furiko batch/v1 Scheduling Cron expressions Cron timezone ( not officially supported ) Scheduling constraints Cron load balancing Forbid concurrent with adhoc execution Back-scheduling (via startingDeadlineSeconds ) Enqueue jobs for later Task Execution Retries using separate Pods Pending timeouts for dead nodes Multiple parallel Pods per Job Parallel expansion by list and matrix Custom task executors (planned) General Automatic cleanup with TTL Parameterization of job inputs","title":"Features"},{"location":"guide/features/#features","text":"Furiko specializes in time-based scheduling of scheduled and adhoc jobs. The following is a non-exhaustive list of features and enhancements offered by Furiko.","title":"Features"},{"location":"guide/features/#feature-list","text":"","title":"Feature List"},{"location":"guide/features/#timezone-aware-cron-scheduling","text":"Furiko allows users to specify a cron schedule to automatically schedule jobs, supporting up to a per-second granularity. Furiko also offers native support for scheduling jobs on a cron schedule with timezones . This allows users to specify their cron schedule in a timezone that is familiar to them, or one that is standardized in their application. If not specified, Furiko also supports specifying a cluster-wide default timezone for all cron schedules.","title":"Timezone-aware Cron Scheduling"},{"location":"guide/features/#cluster-wide-load-balancing","text":"Furiko offers a unique, extended cron syntax that may drastically improve the performance of running distributed cron at scale. For example, specifying H/5 * * * * means to run every 5 minutes, on a \"random\" minute/second within the 5 minute range. This helps to avoid thundering herd effects in clusters when thousands of other jobs also start at the exact same time, evenly spreading out job executions which reduces waiting duration and impact on other downstream dependencies.","title":"Cluster-wide Load Balancing"},{"location":"guide/features/#strong-concurrency-handling","text":"Furiko introduces stronger handling of multiple concurrent jobs . Using the Forbid concurrency policy, Furiko takes a very strict approach to ensuring that multiple jobs will never be started at the same time, free of race conditions. In addition, adhoc jobs executed will be subject to the same concurrency policy as if it were scheduled automatically, which may help prevent untimely incidents.","title":"Strong Concurrency Handling"},{"location":"guide/features/#scheduling-future-executions","text":"Furiko allows ad-hoc jobs to be queued for execution at a later time , or to start a new job execution immediately once the current job execution is finished.","title":"Scheduling Future Executions"},{"location":"guide/features/#preventing-missed-and-double-executions","text":"Furiko provides strong guarantees to schedule jobs even in the face of failure, and is able to prevent both double runs and missed runs in many cases. Furiko prevents double scheduled runs by using deterministic name formats . It also prevents missed runs using back-scheduling to tolerate short downtime, allowing the cluster administrator to safely restart or upgrade Furiko at any time.","title":"Preventing Missed and Double Executions"},{"location":"guide/features/#enhanced-timeout-handling","text":"Furiko offers additional timeouts that can be used during a job execution. For example, pending timeouts help to gracefully handle node outages instead of hanging on a single execution, ensuring forward progress for job configs that are automatically scheduled.","title":"Enhanced Timeout Handling"},{"location":"guide/features/#job-options","text":"Furiko allows you to use JobConfigs as templates for Jobs, by passing parameters into the Job. The JobConfig can be parameterized with Job Options , which defines a structured input that will be validated and substituted at runtime.","title":"Job Options"},{"location":"guide/features/#running-multiple-parallel-tasks-per-job","text":"Furiko supports parallelism of tasks within a Job, which supports index number-based, index key-based or matrix-based expansion of tasks that will be run in parallel. A completion strategy can also be defined that will automatically terminate all parallel tasks when certain conditions are met.","title":"Running multiple parallel tasks per Job"},{"location":"guide/features/#federation-across-multiple-clusters","text":"Furiko also provides add-on support for federating, cordoning and draining of JobConfigs across multiple Kubernetes clusters safely. Info This feature is currently planned in the Roadmap .","title":"Federation Across Multiple Clusters"},{"location":"guide/features/#monitoring-notifications-and-telemetry","text":"Furiko also provides add-on support for monitoring and notifications of job executions and failures, via a variety of methods including Prometheus metrics, Slack webhooks and email notifications. In addition, Furiko provides alternative API servers to query, store and analyze large amounts of job executions per day, reducing the stress on kube-apiserver . Info This feature is currently planned in the Roadmap .","title":"Monitoring, Notifications and Telemetry"},{"location":"guide/features/#comparison-with-batchv1","text":"The following is a short comparison between Furiko and batch/v1 Jobs and CronJobs : Feature Furiko batch/v1 Scheduling Cron expressions Cron timezone ( not officially supported ) Scheduling constraints Cron load balancing Forbid concurrent with adhoc execution Back-scheduling (via startingDeadlineSeconds ) Enqueue jobs for later Task Execution Retries using separate Pods Pending timeouts for dead nodes Multiple parallel Pods per Job Parallel expansion by list and matrix Custom task executors (planned) General Automatic cleanup with TTL Parameterization of job inputs","title":"Comparison with batch/v1"},{"location":"guide/overview/","text":"Furiko Furiko is a cloud-native, enterprise-level cron and adhoc job platform for Kubernetes. The word \"furiko\" (\u632f\u308a\u5b50) means \"pendulum\" in Japanese. Introduction Furiko is a Kubernetes-native operator for managing, scheduling and executing scheduled and adhoc jobs and workflows. It aims to be a general-purpose job platform that supports a diverse range of use cases, including cron jobs, parallel batch processing, workflow automation, etc. Furiko is built from the beginning to support enterprise-level use cases and running self-hosted in a private Kubernetes cluster, supporting users across a large organization. Some use cases that are perfect for Furiko include: Cron-based scheduling massive amounts of periodic jobs per day in a large organization Scheduling some jobs to run once at a later time with some parameters Starting multiple jobs to execute one after another, once the previous job has finished Running multiple parallel, indexed tasks within a single job execution Event-driven, offline/asynchronous job processing Building a platform to automate business operations via form-based inputs (with Furiko as the job engine) Why Choose Furiko There are several other workflow platforms available for Kubernetes today. Furiko specializes in time-based scheduling , taking an emphasis on correctness and scale , providing an out-of-the-box solution to run massive amounts of job executions every day without missing a beat. Initially developed as an internal project in Shopee, Furiko prides itself on being optimized to support millions of job executions per day across multiple clusters internally, while being expressive enough to support a multitude of business use cases. At the same time, Furiko offers high consistency and reliability in the face of failure or restarts to prevent double runs and missed runs. Furiko is designed to be used both as a standalone platform, or as a framework for more complex platforms that build on top of Furiko, such as IT automation or customer service portals. More details can be found on the Features page. Info Furiko is currently in ALPHA and APIs may change at any time, so use it at your own risk. License Furiko is licensed under the Apache License, Version 2.0 . Logo is designed by Duan Weiwei, and is distributed under CC-BY 4.0 . NOTE : Although started within the company, Furiko is not an official Shopee project or product .","title":"Overview"},{"location":"guide/overview/#furiko","text":"Furiko is a cloud-native, enterprise-level cron and adhoc job platform for Kubernetes. The word \"furiko\" (\u632f\u308a\u5b50) means \"pendulum\" in Japanese.","title":"Furiko"},{"location":"guide/overview/#introduction","text":"Furiko is a Kubernetes-native operator for managing, scheduling and executing scheduled and adhoc jobs and workflows. It aims to be a general-purpose job platform that supports a diverse range of use cases, including cron jobs, parallel batch processing, workflow automation, etc. Furiko is built from the beginning to support enterprise-level use cases and running self-hosted in a private Kubernetes cluster, supporting users across a large organization. Some use cases that are perfect for Furiko include: Cron-based scheduling massive amounts of periodic jobs per day in a large organization Scheduling some jobs to run once at a later time with some parameters Starting multiple jobs to execute one after another, once the previous job has finished Running multiple parallel, indexed tasks within a single job execution Event-driven, offline/asynchronous job processing Building a platform to automate business operations via form-based inputs (with Furiko as the job engine)","title":"Introduction"},{"location":"guide/overview/#why-choose-furiko","text":"There are several other workflow platforms available for Kubernetes today. Furiko specializes in time-based scheduling , taking an emphasis on correctness and scale , providing an out-of-the-box solution to run massive amounts of job executions every day without missing a beat. Initially developed as an internal project in Shopee, Furiko prides itself on being optimized to support millions of job executions per day across multiple clusters internally, while being expressive enough to support a multitude of business use cases. At the same time, Furiko offers high consistency and reliability in the face of failure or restarts to prevent double runs and missed runs. Furiko is designed to be used both as a standalone platform, or as a framework for more complex platforms that build on top of Furiko, such as IT automation or customer service portals. More details can be found on the Features page. Info Furiko is currently in ALPHA and APIs may change at any time, so use it at your own risk.","title":"Why Choose Furiko"},{"location":"guide/overview/#license","text":"Furiko is licensed under the Apache License, Version 2.0 . Logo is designed by Duan Weiwei, and is distributed under CC-BY 4.0 . NOTE : Although started within the company, Furiko is not an official Shopee project or product .","title":"License"},{"location":"guide/contributing/overview/","text":"Contribution Guide We're so excited that you want to contribute to Furiko! We will briefly outline the different ways you can contribute to the project. Roadmap Before contributing, refer to the Roadmap which covers planned features and developments for Furiko. Core Kubernetes Operators We welcome all code contributions to the core Furiko Kubernetes operators. This includes the following: Execution : Configuration and execution of scheduled and adhoc jobs. Federation (WIP): Add-on for managing jobs across multiple Kubernetes clusters. Telemetry (WIP): Add-on for notifications, monitoring and telemetry of jobs and executions. The code for the Kubernetes operators lives in https://github.com/furiko-io/furiko . For more information, refer to the Contribution Guide . We also welcome design proposals, feature requests and ideas for the scope of Furiko. Please head over to the issue tracker on GitHub and open an issue to discuss your idea there. Command-Line Tools The furiko CLI tool is also located in the main Furiko repository . For more information, refer to the Contribution Guide . User Interface We currently don't have a user interface for Furiko. If you have experience in building performant and complex web applications, we would love to hear from you. Documentation You can also contribute to this very site! If you find that any documentation is lacking or unclear, or if you simply found a typo, we are more than happe to accept contributions big or small. The code for this website lives in https://github.com/furiko-io/docs . For more information, refer to the Contribution Guide . Testing No project can succeed without the confidence of tests, including both automated tests and the hard work of QA testers. Some areas that we would love contributions in include: Setting up test infrastructure Designing end-to-end tests for controllers and webhooks Increasing code coverage of unit tests Stress/scalability testing Security testing and fuzzing Other Ways to Contribute There are tons of other ways to contribute to the project. Report a bug : You can report a bug via the issue tracker on GitHub . Request a feature : You can also request features via the issue tracker on GitHub .","title":"Contribution Guide"},{"location":"guide/contributing/overview/#contribution-guide","text":"We're so excited that you want to contribute to Furiko! We will briefly outline the different ways you can contribute to the project.","title":"Contribution Guide"},{"location":"guide/contributing/overview/#roadmap","text":"Before contributing, refer to the Roadmap which covers planned features and developments for Furiko.","title":"Roadmap"},{"location":"guide/contributing/overview/#core-kubernetes-operators","text":"We welcome all code contributions to the core Furiko Kubernetes operators. This includes the following: Execution : Configuration and execution of scheduled and adhoc jobs. Federation (WIP): Add-on for managing jobs across multiple Kubernetes clusters. Telemetry (WIP): Add-on for notifications, monitoring and telemetry of jobs and executions. The code for the Kubernetes operators lives in https://github.com/furiko-io/furiko . For more information, refer to the Contribution Guide . We also welcome design proposals, feature requests and ideas for the scope of Furiko. Please head over to the issue tracker on GitHub and open an issue to discuss your idea there.","title":"Core Kubernetes Operators"},{"location":"guide/contributing/overview/#command-line-tools","text":"The furiko CLI tool is also located in the main Furiko repository . For more information, refer to the Contribution Guide .","title":"Command-Line Tools"},{"location":"guide/contributing/overview/#user-interface","text":"We currently don't have a user interface for Furiko. If you have experience in building performant and complex web applications, we would love to hear from you.","title":"User Interface"},{"location":"guide/contributing/overview/#documentation","text":"You can also contribute to this very site! If you find that any documentation is lacking or unclear, or if you simply found a typo, we are more than happe to accept contributions big or small. The code for this website lives in https://github.com/furiko-io/docs . For more information, refer to the Contribution Guide .","title":"Documentation"},{"location":"guide/contributing/overview/#testing","text":"No project can succeed without the confidence of tests, including both automated tests and the hard work of QA testers. Some areas that we would love contributions in include: Setting up test infrastructure Designing end-to-end tests for controllers and webhooks Increasing code coverage of unit tests Stress/scalability testing Security testing and fuzzing","title":"Testing"},{"location":"guide/contributing/overview/#other-ways-to-contribute","text":"There are tons of other ways to contribute to the project. Report a bug : You can report a bug via the issue tracker on GitHub . Request a feature : You can also request features via the issue tracker on GitHub .","title":"Other Ways to Contribute"},{"location":"guide/contributing/roadmap/","text":"Roadmap The roadmap is primarily maintained on Google Docs: https://docs.google.com/document/d/1p5nZR7pNEGtXILUevnH0UHfRlnjhKwJjT8HlBcI8e5I/edit?usp=sharing Contributing Ideas To suggest ideas or request features for Furiko, we suggest to either comment directly in the Google Doc, or to open a GitHub issue on the main Furiko repository.","title":"Roadmap"},{"location":"guide/contributing/roadmap/#roadmap","text":"The roadmap is primarily maintained on Google Docs: https://docs.google.com/document/d/1p5nZR7pNEGtXILUevnH0UHfRlnjhKwJjT8HlBcI8e5I/edit?usp=sharing","title":"Roadmap"},{"location":"guide/contributing/roadmap/#contributing-ideas","text":"To suggest ideas or request features for Furiko, we suggest to either comment directly in the Google Doc, or to open a GitHub issue on the main Furiko repository.","title":"Contributing Ideas"},{"location":"guide/development/overview/","text":"Developer Guide This section contains developer guides. Read on if you are interested in the inner workings of Furiko as well.","title":"Developer Guide"},{"location":"guide/development/overview/#developer-guide","text":"This section contains developer guides. Read on if you are interested in the inner workings of Furiko as well.","title":"Developer Guide"},{"location":"guide/development/architecture/","text":"Architecture This section will discuss the architecture of Furiko. Component Types Furiko's architecture consists of several types of components: Controller Managers : Controller manager that combines multiple controllers together. Webhook Services : Kubernetes services that serve as Admission Webhook servers. ExecutionController The ExecutionController is responsible for managing and interacting with Furiko Execution CRDs. ExecutionWebhook The ExecutionWebhook is responsible for enforcing semantics and populating of several convenience fields for Furiko Execution CRDs.","title":"Architecture"},{"location":"guide/development/architecture/#architecture","text":"This section will discuss the architecture of Furiko.","title":"Architecture"},{"location":"guide/development/architecture/#component-types","text":"Furiko's architecture consists of several types of components: Controller Managers : Controller manager that combines multiple controllers together. Webhook Services : Kubernetes services that serve as Admission Webhook servers.","title":"Component Types"},{"location":"guide/development/architecture/#executioncontroller","text":"The ExecutionController is responsible for managing and interacting with Furiko Execution CRDs.","title":"ExecutionController"},{"location":"guide/development/architecture/#executionwebhook","text":"The ExecutionWebhook is responsible for enforcing semantics and populating of several convenience fields for Furiko Execution CRDs.","title":"ExecutionWebhook"},{"location":"guide/development/architecture/execution-controller/","text":"ExecutionController ExecutionController consists of several controllers responsible for managing Jobs and JobConfigs . This page will attempt to discuss some of the inner workings of the individual controllers and webhooks, explaining how some guarantees are provided. A large part of the design was inspired from a chapter of Google's SRE Book ( Distributed Periodic Scheduling with Cron ) and other platforms including Rundeck . CronController The CronController is responsible for scheduling Jobs based on their JobConfig's schedule. Every second, the CronController iterates through all JobConfigs, checking if any of them are due for scheduling. It uses an in-memory cache to store the next expected schedule time for all JobConfigs to avoid expensive CPU computation every second. If a JobConfig is due to be scheduled, additional checks like concurrency policy will be performed before actually creating the Job object. The CronController uses deterministic Job name format: <JOBCONFIG_NAME>.<UNIX_TIMESTAMP> (e.g. jobconfig-sample.1646586360 ). This helps prevent duplicate Jobs from being started in the event of a retryable error, or if the controller was restarted in the middle of scheduling a JobConfig. The CronController is also able to detect if any schedules were missed and back-schedule them accordingly . JobQueueController The JobQueueController is responsible for starting Jobs that were created by setting the status.StartTime field. This process of \"starting\" a Job is so that we can: Ensure that Jobs do not violate their concurrency policy in any way, by maintaining and performing atomic operations to an in-memory ActiveJobStore . Start any Jobs that were deferred to run in the future. Ensure to start multiple Queued jobs in the order that they were created. The JobQueueController is able to parallelize to multiple workers (goroutines) and each goroutine works on a separate subset of Jobs partitioned by their JobConfig. Thanks to workqueue , we can ensure that no two goroutines will be working on the same set of Jobs concurrently, and so we can process all Jobs in a linearizable fashion without violating any concurrency or ordering guarantees. ActiveJobStore The JobQueueController interacts with an in-memory store called the ActiveJobStore , which is responsible for maintaining the source of truth of active job counts per JobConfig. The ActiveJobStore acts as a global synchronizer for the entire cluster, and works in the following manner: Only the leading ExecutionController has an in-memory ActiveJobStore, otherwise non-leaders should not attempt to populate the store; any past leader which lost its lease MUST give up control immediately. Upon getting elected as leader, the contents of the ActiveJobStore is rehydrated from the current state of the cluster, by listing all current Jobs in kube-apiserver and counting by JobConfig. The ActiveJobStore also has an internal informer to get notified when a Job transitions between active and inactive states, and update its counts internally. The strong assumption here is that a Job does not transition from inactive back to active, which provides the concurrency guarantees explained below. Once store rehydration is completed, the other controllers are allowed to start. Before starting a Job, the ActiveJobStore's counter must be incremented in a check-and-set fashion to avoid race conditions. Since informer events are received after the actual state update in kube-apiserver, there is a possibility of a delayed update from active -> inactive , which prevents a new Job from being started despite previous Jobs already having been completed. On the other hand, there is no way for two Jobs to be started concurrently, because the ActiveJobStore expects that any transitions from inactive -> active MUST be explicitly written to the store, rather than read via a reconcile. This is the guarantee that the ActiveJobStore provides. Info Before the introduction of the JobQueueController to sequentially process all Jobs for starting on a per-JobConfig basis, multiple threads could update the ActiveJobStore at the same time, resulting in TOCTTOU race conditions . However, after JobQueueController was introduced, the likelihood of race conditions via non-atomic CAS operations falls to 0 and using check-and-set is actually no longer required, but remains as an additional safety guarantee. JobController The JobController is responsible for reconciling Jobs after they have been started by the JobQueueController. It performs roughly the following actions in order: Create any tasks that have yet to be created, or wait for a retry delay if set. Perform context variable and job option substitution as tasks are created. Check if any tasks have exceeded its pending timeout, and if so, kill those tasks. Propagate the Job's kill timestamp, if any, to all its unfinished tasks. Check if tasks are still alive beyond a timeout, and use API deletion to kill any unterminated tasks being killed. Reconcile status from all tasks. Delete the Job if it is already finished and has lived beyond its TTL. Ensure all tasks are deleted, reconcile the final status, and remove the finalizer for deletion to proceed. Some key notes for this controller: Jobs will only create tasks when they are started, otherwise they will perform no write actions. It is the responsibility of the JobQueueController to start only allowed Jobs in the order that they should be started in. Tasks created by Jobs will follow a deterministic name format: <JOB_NAME>.<RETRY_INDEX> (e.g. jobconfig-sample-c6k89.1 ). If a task already exists with the given name, the controller assumes that its status is out-of-date and will \"adopt\" the task. This prevents duplicate tasks from being created. JobConfigController The JobConfigController is responsible for reconciling the status of JobConfigs from its downstream Jobs. It is a somewhat straightforward controller, performing only the following actions: Reconcile the list of queued and active Jobs. Update the JobConfig's LastScheduleTime which would be used for back-scheduling.","title":"ExecutionController"},{"location":"guide/development/architecture/execution-controller/#executioncontroller","text":"ExecutionController consists of several controllers responsible for managing Jobs and JobConfigs . This page will attempt to discuss some of the inner workings of the individual controllers and webhooks, explaining how some guarantees are provided. A large part of the design was inspired from a chapter of Google's SRE Book ( Distributed Periodic Scheduling with Cron ) and other platforms including Rundeck .","title":"ExecutionController"},{"location":"guide/development/architecture/execution-controller/#croncontroller","text":"The CronController is responsible for scheduling Jobs based on their JobConfig's schedule. Every second, the CronController iterates through all JobConfigs, checking if any of them are due for scheduling. It uses an in-memory cache to store the next expected schedule time for all JobConfigs to avoid expensive CPU computation every second. If a JobConfig is due to be scheduled, additional checks like concurrency policy will be performed before actually creating the Job object. The CronController uses deterministic Job name format: <JOBCONFIG_NAME>.<UNIX_TIMESTAMP> (e.g. jobconfig-sample.1646586360 ). This helps prevent duplicate Jobs from being started in the event of a retryable error, or if the controller was restarted in the middle of scheduling a JobConfig. The CronController is also able to detect if any schedules were missed and back-schedule them accordingly .","title":"CronController"},{"location":"guide/development/architecture/execution-controller/#jobqueuecontroller","text":"The JobQueueController is responsible for starting Jobs that were created by setting the status.StartTime field. This process of \"starting\" a Job is so that we can: Ensure that Jobs do not violate their concurrency policy in any way, by maintaining and performing atomic operations to an in-memory ActiveJobStore . Start any Jobs that were deferred to run in the future. Ensure to start multiple Queued jobs in the order that they were created. The JobQueueController is able to parallelize to multiple workers (goroutines) and each goroutine works on a separate subset of Jobs partitioned by their JobConfig. Thanks to workqueue , we can ensure that no two goroutines will be working on the same set of Jobs concurrently, and so we can process all Jobs in a linearizable fashion without violating any concurrency or ordering guarantees.","title":"JobQueueController"},{"location":"guide/development/architecture/execution-controller/#activejobstore","text":"The JobQueueController interacts with an in-memory store called the ActiveJobStore , which is responsible for maintaining the source of truth of active job counts per JobConfig. The ActiveJobStore acts as a global synchronizer for the entire cluster, and works in the following manner: Only the leading ExecutionController has an in-memory ActiveJobStore, otherwise non-leaders should not attempt to populate the store; any past leader which lost its lease MUST give up control immediately. Upon getting elected as leader, the contents of the ActiveJobStore is rehydrated from the current state of the cluster, by listing all current Jobs in kube-apiserver and counting by JobConfig. The ActiveJobStore also has an internal informer to get notified when a Job transitions between active and inactive states, and update its counts internally. The strong assumption here is that a Job does not transition from inactive back to active, which provides the concurrency guarantees explained below. Once store rehydration is completed, the other controllers are allowed to start. Before starting a Job, the ActiveJobStore's counter must be incremented in a check-and-set fashion to avoid race conditions. Since informer events are received after the actual state update in kube-apiserver, there is a possibility of a delayed update from active -> inactive , which prevents a new Job from being started despite previous Jobs already having been completed. On the other hand, there is no way for two Jobs to be started concurrently, because the ActiveJobStore expects that any transitions from inactive -> active MUST be explicitly written to the store, rather than read via a reconcile. This is the guarantee that the ActiveJobStore provides. Info Before the introduction of the JobQueueController to sequentially process all Jobs for starting on a per-JobConfig basis, multiple threads could update the ActiveJobStore at the same time, resulting in TOCTTOU race conditions . However, after JobQueueController was introduced, the likelihood of race conditions via non-atomic CAS operations falls to 0 and using check-and-set is actually no longer required, but remains as an additional safety guarantee.","title":"ActiveJobStore"},{"location":"guide/development/architecture/execution-controller/#jobcontroller","text":"The JobController is responsible for reconciling Jobs after they have been started by the JobQueueController. It performs roughly the following actions in order: Create any tasks that have yet to be created, or wait for a retry delay if set. Perform context variable and job option substitution as tasks are created. Check if any tasks have exceeded its pending timeout, and if so, kill those tasks. Propagate the Job's kill timestamp, if any, to all its unfinished tasks. Check if tasks are still alive beyond a timeout, and use API deletion to kill any unterminated tasks being killed. Reconcile status from all tasks. Delete the Job if it is already finished and has lived beyond its TTL. Ensure all tasks are deleted, reconcile the final status, and remove the finalizer for deletion to proceed. Some key notes for this controller: Jobs will only create tasks when they are started, otherwise they will perform no write actions. It is the responsibility of the JobQueueController to start only allowed Jobs in the order that they should be started in. Tasks created by Jobs will follow a deterministic name format: <JOB_NAME>.<RETRY_INDEX> (e.g. jobconfig-sample-c6k89.1 ). If a task already exists with the given name, the controller assumes that its status is out-of-date and will \"adopt\" the task. This prevents duplicate tasks from being created.","title":"JobController"},{"location":"guide/development/architecture/execution-controller/#jobconfigcontroller","text":"The JobConfigController is responsible for reconciling the status of JobConfigs from its downstream Jobs. It is a somewhat straightforward controller, performing only the following actions: Reconcile the list of queued and active Jobs. Update the JobConfig's LastScheduleTime which would be used for back-scheduling.","title":"JobConfigController"},{"location":"guide/development/architecture/execution-webhook/","text":"ExecutionWebhook ExecutionWebhook consists of several dynamic admission webhooks for validating and mutating Jobs and JobConfigs . The purpose of the webhook server is to provide sanity checks to ensure that malformed resources will not be admitted to the cluster, as well as to provide certain convenience functions via mutating admission webhooks. JobConfig Bumping scheduleNotBefore Upon updating the JobConfig's schedule , the scheduleNotBefore timestamp will also be bumped to the current time via a mutating admission webhook. This prevents back-scheduling when a job's interval was shortened or re-enabled after being disabled for a period of time. Job Rejecting Forbid concurrency policy When creating a new Job, the validating admission webhook will attempt to forbid incoming Jobs on a best-effort basis, by checking the parent JobConfig's status. If two Jobs are started simultaneously, it may be possible that the JobConfig's status is not up-to-date yet, and the webhook may incorrectly admit the request. Only the leading controller is able to 100% determine if the Job will not violate the concurrency policy of the JobConfig by using an atomic in-memory store. As such, the webhook purely performs a best-effort rejection of the Job, but defers the actual admission of the Job to the JobQueueController . This means that the Job could be terminated with an abnormal AdmissionError after determining that it is not safe to admit it for execution. Preparing optionValues When a Job is created, the mutating admission webhook evaluates all fields specified in optionValues before populating the final set of substitutions into substitutions . This means that optionValues are evaluated only at creation time, and if the JobConfig is modified while the Job is active, the changes will not take effect until we start a subsequent Job.","title":"ExecutionWebhook"},{"location":"guide/development/architecture/execution-webhook/#executionwebhook","text":"ExecutionWebhook consists of several dynamic admission webhooks for validating and mutating Jobs and JobConfigs . The purpose of the webhook server is to provide sanity checks to ensure that malformed resources will not be admitted to the cluster, as well as to provide certain convenience functions via mutating admission webhooks.","title":"ExecutionWebhook"},{"location":"guide/development/architecture/execution-webhook/#jobconfig","text":"","title":"JobConfig"},{"location":"guide/development/architecture/execution-webhook/#bumping-schedulenotbefore","text":"Upon updating the JobConfig's schedule , the scheduleNotBefore timestamp will also be bumped to the current time via a mutating admission webhook. This prevents back-scheduling when a job's interval was shortened or re-enabled after being disabled for a period of time.","title":"Bumping scheduleNotBefore"},{"location":"guide/development/architecture/execution-webhook/#job","text":"","title":"Job"},{"location":"guide/development/architecture/execution-webhook/#rejecting-forbid-concurrency-policy","text":"When creating a new Job, the validating admission webhook will attempt to forbid incoming Jobs on a best-effort basis, by checking the parent JobConfig's status. If two Jobs are started simultaneously, it may be possible that the JobConfig's status is not up-to-date yet, and the webhook may incorrectly admit the request. Only the leading controller is able to 100% determine if the Job will not violate the concurrency policy of the JobConfig by using an atomic in-memory store. As such, the webhook purely performs a best-effort rejection of the Job, but defers the actual admission of the Job to the JobQueueController . This means that the Job could be terminated with an abnormal AdmissionError after determining that it is not safe to admit it for execution.","title":"Rejecting Forbid concurrency policy"},{"location":"guide/development/architecture/execution-webhook/#preparing-optionvalues","text":"When a Job is created, the mutating admission webhook evaluates all fields specified in optionValues before populating the final set of substitutions into substitutions . This means that optionValues are evaluated only at creation time, and if the JobConfig is modified while the Job is active, the changes will not take effect until we start a subsequent Job.","title":"Preparing optionValues"},{"location":"guide/execution/concepts/","text":"Execution Concepts Furiko Execution is the heart of the Furiko platform, and consists of components to configure scheduled jobs and trigger ad-hoc executions. JobConfig A JobConfig describes an automation job to be run. It creates and controls Job children objects. A JobConfig can be automatically scheduled to be create Jobs based on a cron schedule, or executed on an ad-hoc basis. You can also parameterize a JobConfig to take in different variables which will be substituted into the job template at runtime. Job A Job represents a single job execution, analogous to a batch/v1 Job . As the name suggests, a Job's parent is a JobConfig. A Job's spec is local to the particular job execution. Modifying the JobConfig after the Job has been created will not affect previously created Jobs. Info A Job can also be created independently of a JobConfig controller, although this is not recommended. Doing so will disable certain features and guarantees that are provided by a parent JobConfig controller. Task A Task represents a Job's actual manifestation of task execution. A Job may create one or more Tasks, which can represent both parallel tasks being executed (see Parallel Execution) or individual retries (see Timeouts and Retries ). A Task is an abstract representation of a workload, such as a Pod . Currently, the only supported task executor is a Pod, but more task executors are planned in the future.","title":"Concepts"},{"location":"guide/execution/concepts/#execution-concepts","text":"Furiko Execution is the heart of the Furiko platform, and consists of components to configure scheduled jobs and trigger ad-hoc executions.","title":"Execution Concepts"},{"location":"guide/execution/concepts/#jobconfig","text":"A JobConfig describes an automation job to be run. It creates and controls Job children objects. A JobConfig can be automatically scheduled to be create Jobs based on a cron schedule, or executed on an ad-hoc basis. You can also parameterize a JobConfig to take in different variables which will be substituted into the job template at runtime.","title":"JobConfig"},{"location":"guide/execution/concepts/#job","text":"A Job represents a single job execution, analogous to a batch/v1 Job . As the name suggests, a Job's parent is a JobConfig. A Job's spec is local to the particular job execution. Modifying the JobConfig after the Job has been created will not affect previously created Jobs. Info A Job can also be created independently of a JobConfig controller, although this is not recommended. Doing so will disable certain features and guarantees that are provided by a parent JobConfig controller.","title":"Job"},{"location":"guide/execution/concepts/#task","text":"A Task represents a Job's actual manifestation of task execution. A Job may create one or more Tasks, which can represent both parallel tasks being executed (see Parallel Execution) or individual retries (see Timeouts and Retries ). A Task is an abstract representation of a workload, such as a Pod . Currently, the only supported task executor is a Pod, but more task executors are planned in the future.","title":"Task"},{"location":"guide/execution/job/","text":"Job This page describes how to configure a Job. You can find a full sample configuration here . Task Executor A Job creates one or more tasks during its lifecycle. Each task corresponds to a single parallel and retry index for the Job. The Job needs a template which describes how to create tasks. You can specify the task template under .spec.taskTemplate , which supports the following fields (exactly one must be specified): pod : Create the task as a Pod. This template is a PodTemplateSpec . Currently, only the pod task executor is supported. For more details on the task executor interface, see Task Executors . Running Adhoc Jobs A Job can be created and started automatically by the JobConfig controller, but it can also be started on an adhoc basis. Jobs created from a JobConfig will inherit its template, are subject to the JobConfig's concurrency policies, and can utilize the Job Options defined in the JobConfig's spec. For more details, see Adhoc Execution . Start Policy When a Job is created, it may not necessarily start executing right away. This is defined by the .spec.startPolicy . For more details, see Start Policy . Timeouts and Retries Furiko provides several mechanisms to impose timeouts and retrying of failed Jobs. For more details see Timeouts and Retries . Substitutions Furiko exposes a substitutions field, so that the JobController knows how to substitute context variables into the task template at runtime. This field is also used when creating a Job from a JobConfig, for example via Job Options . All substitutions should be a key-value map of strings: Example JobSpec spec : substitutions : option.user-name : \"John Smith\" option.job-types : \"adhoc,scheduled\" For more details, see Context Variables . Garbage Collection Jobs should not live for too long after they are finished. Furiko imposes a time-to-live (TTL) on all Jobs after they are finished, so that the Kubernetes will not be too overloaded and controllers will be more responsive in general. For more details see Garbage Collection .","title":"Job"},{"location":"guide/execution/job/#job","text":"This page describes how to configure a Job. You can find a full sample configuration here .","title":"Job"},{"location":"guide/execution/job/#task-executor","text":"A Job creates one or more tasks during its lifecycle. Each task corresponds to a single parallel and retry index for the Job. The Job needs a template which describes how to create tasks. You can specify the task template under .spec.taskTemplate , which supports the following fields (exactly one must be specified): pod : Create the task as a Pod. This template is a PodTemplateSpec . Currently, only the pod task executor is supported. For more details on the task executor interface, see Task Executors .","title":"Task Executor"},{"location":"guide/execution/job/#running-adhoc-jobs","text":"A Job can be created and started automatically by the JobConfig controller, but it can also be started on an adhoc basis. Jobs created from a JobConfig will inherit its template, are subject to the JobConfig's concurrency policies, and can utilize the Job Options defined in the JobConfig's spec. For more details, see Adhoc Execution .","title":"Running Adhoc Jobs"},{"location":"guide/execution/job/#start-policy","text":"When a Job is created, it may not necessarily start executing right away. This is defined by the .spec.startPolicy . For more details, see Start Policy .","title":"Start Policy"},{"location":"guide/execution/job/#timeouts-and-retries","text":"Furiko provides several mechanisms to impose timeouts and retrying of failed Jobs. For more details see Timeouts and Retries .","title":"Timeouts and Retries"},{"location":"guide/execution/job/#substitutions","text":"Furiko exposes a substitutions field, so that the JobController knows how to substitute context variables into the task template at runtime. This field is also used when creating a Job from a JobConfig, for example via Job Options . All substitutions should be a key-value map of strings: Example JobSpec spec : substitutions : option.user-name : \"John Smith\" option.job-types : \"adhoc,scheduled\" For more details, see Context Variables .","title":"Substitutions"},{"location":"guide/execution/job/#garbage-collection","text":"Jobs should not live for too long after they are finished. Furiko imposes a time-to-live (TTL) on all Jobs after they are finished, so that the Kubernetes will not be too overloaded and controllers will be more responsive in general. For more details see Garbage Collection .","title":"Garbage Collection"},{"location":"guide/execution/job/adhoc-execution/","text":"Adhoc Execution This page will discuss how to run once-off, or adhoc Job executions. An adhoc Job is one that is not scheduled automatically (e.g. from a cron schedule), but rather started explicitly (such as via user creation or external triggers). Using the CLI The easiest way to run an adhoc Job execution is to use the furiko CLI tool : It is recommended to use furiko run , which supports the following features: Interactive prompt for option values (suppress with --use-default-options ) Specify future timestamp to start after (with --at ) Creating a Job from a JobConfig A Job is typically created from a JobConfig, since the JobConfig controller groups together multiple Job objects and controls their lifecycle and behavior. configName To create a Job from a JobConfig, the simplest way is to specify the configName of a JobConfig in the same namespace: Example: Creating Jobs with configName apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- username: bob The webhook is responsible for transforming the Job creation request, so that all fields will be populated from the JobConfig. The end result would be the following, extremely comprehensive Job configuration. Some fields are explained below. Example: Final Result after transforming Jobs with configName apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : annotations : # Internal annotation used to \"snapshot\" the JobOptionsSpec of the JobConfig. execution.furiko.io/options-spec-hash : 91305051fc67f709 creationTimestamp : \"2022-03-15T07:35:21Z\" finalizers : # This finalizer is needed by the JobController. - execution.furiko.io/delete-dependents-finalizer generateName : jobconfig-sample- generation : 1 labels : # Internal label used to identify owner JobConfig. execution.furiko.io/job-config-uid : cd346c6f-4493-42b5-a813-8787a68ec74c name : jobconfig-sample-p8fzq namespace : default # Owner reference of JobConfig controller. ownerReferences : - apiVersion : execution.furiko.io/v1alpha1 blockOwnerDeletion : true controller : true kind : JobConfig name : jobconfig-sample uid : cd346c6f-4493-42b5-a813-8787a68ec74c resourceVersion : \"585289534\" uid : f02bf84c-ab06-4121-9c7f-be50ff61de72 spec : type : Adhoc optionValues : '{\"username\":\"bob\"}' startPolicy : # Inherited from the JobConfig's concurrency.policy. concurrencyPolicy : Forbid substitutions : # Addition of jobconfig context variables. jobconfig.cron_schedule : H/15 * * * * jobconfig.name : jobconfig-sample jobconfig.namespace : default jobconfig.timezone : Asia/Singapore # Evaluted option variables. option.image-tag : latest option.username : Example User # Inherited from the JobConfig's spec.template. template : maxAttempts : 1 taskTemplate : pod : metadata : {} spec : containers : - args : - echo - Hello world, ${option.username}! env : - name : JOBCONFIG_NAME value : ${jobconfig.name} - name : JOB_NAME value : ${job.name} - name : TASK_NAME value : ${task.name} image : alpine:${option.image-tag} name : job-container resources : limits : cpu : 100m memory : 64Mi restartPolicy : Never ttlSecondsAfterFinished : 3600 optionValues When creating a Job from a JobConfig, you can also specify optionValues which declares the value for each option defined in the JobConfig's Job Options . The optionValues must be a YAML or JSON encoded string of a map from the option's name to the corresponding value. The value may be of any admissible type, depending on the option's `type. Example: Job with optionValues apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- # String and Select options accept only strings. # Numbers or booleans must be quoted in YAML. configName: default-config userCount: \"3\" # Multi options accept a list of strings. userList: - mary - bob - alice # Bool options accept a boolean value. verbose: true If not specified in optionValues , the default value (if any) from the JobConfig will be used. Otherwise, if it is also required , a validation error will be thrown if there is no default and not explicitly defined in optionValues . After evaluation, the final rendered values will be stored inside substitutions . See the previous section for a full example. Concurrency When creating a Job from a JobConfig, it is still subject to the JobConfig's concurrency policy . The startPolicy automatically inherits the JobConfig's concurrency.policy if not defined. However, you can also explicitly specify startPolicy.concurrencyPolicy in the Job's spec, so that it does not follow the JobConfig's actual policy: apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample startPolicy : concurrencyPolicy : Enqueue Assuming the JobConfig uses Forbid , this effectively allows us to schedule an adhoc execution after all currently running Jobs are completed. Info Explicitly specifying startPolicy.concurrencyPolicy in a Job may be a violation of the JobConfig's intended behavior, especially if the Job sets it to Allow even though the JobConfig uses Forbid . Use this with caution. Scheduling Adhoc Future Executions Jobs can also be started in the future, rather than starting immediately upon creation. You can create a Job with startAfter as follows: Example: Creating Job with startAfter apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample startPolicy : concurrencyPolicy : Enqueue startAfter : 2022-03-06T00:27:00+08:00 For more details, see Start Policy . Independent Jobs It is also possible to run Jobs without a JobConfig. However, this is not recommended as it would disable the features mentioned above, including the ability to use optionValues and enforcing concurrency policies. In such a case, you have to explicitly specify the template of the Job yourself. For an example, see Sample Configuration .","title":"Adhoc Execution"},{"location":"guide/execution/job/adhoc-execution/#adhoc-execution","text":"This page will discuss how to run once-off, or adhoc Job executions. An adhoc Job is one that is not scheduled automatically (e.g. from a cron schedule), but rather started explicitly (such as via user creation or external triggers).","title":"Adhoc Execution"},{"location":"guide/execution/job/adhoc-execution/#using-the-cli","text":"The easiest way to run an adhoc Job execution is to use the furiko CLI tool : It is recommended to use furiko run , which supports the following features: Interactive prompt for option values (suppress with --use-default-options ) Specify future timestamp to start after (with --at )","title":"Using the CLI"},{"location":"guide/execution/job/adhoc-execution/#creating-a-job-from-a-jobconfig","text":"A Job is typically created from a JobConfig, since the JobConfig controller groups together multiple Job objects and controls their lifecycle and behavior.","title":"Creating a Job from a JobConfig"},{"location":"guide/execution/job/adhoc-execution/#configname","text":"To create a Job from a JobConfig, the simplest way is to specify the configName of a JobConfig in the same namespace: Example: Creating Jobs with configName apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- username: bob The webhook is responsible for transforming the Job creation request, so that all fields will be populated from the JobConfig. The end result would be the following, extremely comprehensive Job configuration. Some fields are explained below. Example: Final Result after transforming Jobs with configName apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : annotations : # Internal annotation used to \"snapshot\" the JobOptionsSpec of the JobConfig. execution.furiko.io/options-spec-hash : 91305051fc67f709 creationTimestamp : \"2022-03-15T07:35:21Z\" finalizers : # This finalizer is needed by the JobController. - execution.furiko.io/delete-dependents-finalizer generateName : jobconfig-sample- generation : 1 labels : # Internal label used to identify owner JobConfig. execution.furiko.io/job-config-uid : cd346c6f-4493-42b5-a813-8787a68ec74c name : jobconfig-sample-p8fzq namespace : default # Owner reference of JobConfig controller. ownerReferences : - apiVersion : execution.furiko.io/v1alpha1 blockOwnerDeletion : true controller : true kind : JobConfig name : jobconfig-sample uid : cd346c6f-4493-42b5-a813-8787a68ec74c resourceVersion : \"585289534\" uid : f02bf84c-ab06-4121-9c7f-be50ff61de72 spec : type : Adhoc optionValues : '{\"username\":\"bob\"}' startPolicy : # Inherited from the JobConfig's concurrency.policy. concurrencyPolicy : Forbid substitutions : # Addition of jobconfig context variables. jobconfig.cron_schedule : H/15 * * * * jobconfig.name : jobconfig-sample jobconfig.namespace : default jobconfig.timezone : Asia/Singapore # Evaluted option variables. option.image-tag : latest option.username : Example User # Inherited from the JobConfig's spec.template. template : maxAttempts : 1 taskTemplate : pod : metadata : {} spec : containers : - args : - echo - Hello world, ${option.username}! env : - name : JOBCONFIG_NAME value : ${jobconfig.name} - name : JOB_NAME value : ${job.name} - name : TASK_NAME value : ${task.name} image : alpine:${option.image-tag} name : job-container resources : limits : cpu : 100m memory : 64Mi restartPolicy : Never ttlSecondsAfterFinished : 3600","title":"configName"},{"location":"guide/execution/job/adhoc-execution/#optionvalues","text":"When creating a Job from a JobConfig, you can also specify optionValues which declares the value for each option defined in the JobConfig's Job Options . The optionValues must be a YAML or JSON encoded string of a map from the option's name to the corresponding value. The value may be of any admissible type, depending on the option's `type. Example: Job with optionValues apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- # String and Select options accept only strings. # Numbers or booleans must be quoted in YAML. configName: default-config userCount: \"3\" # Multi options accept a list of strings. userList: - mary - bob - alice # Bool options accept a boolean value. verbose: true If not specified in optionValues , the default value (if any) from the JobConfig will be used. Otherwise, if it is also required , a validation error will be thrown if there is no default and not explicitly defined in optionValues . After evaluation, the final rendered values will be stored inside substitutions . See the previous section for a full example.","title":"optionValues"},{"location":"guide/execution/job/adhoc-execution/#concurrency","text":"When creating a Job from a JobConfig, it is still subject to the JobConfig's concurrency policy . The startPolicy automatically inherits the JobConfig's concurrency.policy if not defined. However, you can also explicitly specify startPolicy.concurrencyPolicy in the Job's spec, so that it does not follow the JobConfig's actual policy: apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample startPolicy : concurrencyPolicy : Enqueue Assuming the JobConfig uses Forbid , this effectively allows us to schedule an adhoc execution after all currently running Jobs are completed. Info Explicitly specifying startPolicy.concurrencyPolicy in a Job may be a violation of the JobConfig's intended behavior, especially if the Job sets it to Allow even though the JobConfig uses Forbid . Use this with caution.","title":"Concurrency"},{"location":"guide/execution/job/adhoc-execution/#scheduling-adhoc-future-executions","text":"Jobs can also be started in the future, rather than starting immediately upon creation. You can create a Job with startAfter as follows: Example: Creating Job with startAfter apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample startPolicy : concurrencyPolicy : Enqueue startAfter : 2022-03-06T00:27:00+08:00 For more details, see Start Policy .","title":"Scheduling Adhoc Future Executions"},{"location":"guide/execution/job/adhoc-execution/#independent-jobs","text":"It is also possible to run Jobs without a JobConfig. However, this is not recommended as it would disable the features mentioned above, including the ability to use optionValues and enforcing concurrency policies. In such a case, you have to explicitly specify the template of the Job yourself. For an example, see Sample Configuration .","title":"Independent Jobs"},{"location":"guide/execution/job/force-deletion/","text":"Force Deletion This page describes when and how force deletion is used in the JobController. Motivation When killing a Job , the controller first attempts to use normal deletion to gracefully terminate the task. However, if the node is not responsive (e.g. kubelet is not running, unreachable via network, etc.), such deletion may not be effective, since Kubernetes ensures that the Pod will not be deleted until it has confirmed that the container is fully removed and cleaned up on the node itself. If the JobConfig uses a concurrency policy like Forbid , this may indefinitely block future progress of the JobConfig from creating and starting new Jobs. As such, Furiko supports \"force deletion\" as a last resort, which forcefully deletes the Pod, allowing the Job to be terminated in order for the JobConfig to schedule new Jobs. Deletion Process The \"force deletion\" process is analogous to simply running: kubectl delete pod <pod name> --grace-period=0 --force For more information, see Forced Pod termination on the Kubernetes documentation. Warning The official Kubernetes official documentation mentions the following warning about force deletion (emphasis added): IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion . If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency . Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately. Typically when running cron jobs, force deletion is safe most of the time, and in most cases would prefer availability of scheduling over strict guarantees. However, for jobs that are more crucial, they may choose to disable this functionality and instead trigger notifications to investigate node-level issues. Configuration Task: forbidTaskForceDeletion You can prevent force deletion by specifying forbidTaskForceDeletion: true in the JobTaskSpec, as follows: apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : template : # The normal task template taskTemplate : ... # Forbids force deletion of the task. forbidTaskForceDeletion : true In such a scenario, the controller will simply just wait until the Pod transitions into a terminated state before marking the Job as terminated as well. Global: forceDeleteTaskTimeoutSeconds Specifies the default duration that the controller should wait after the deletionTimestamp of a task before forcefully deleting the task. Set to 0 to disable force deletion globally. For more information, see the Execution Dynamic Config documentation.","title":"Force Deletion"},{"location":"guide/execution/job/force-deletion/#force-deletion","text":"This page describes when and how force deletion is used in the JobController.","title":"Force Deletion"},{"location":"guide/execution/job/force-deletion/#motivation","text":"When killing a Job , the controller first attempts to use normal deletion to gracefully terminate the task. However, if the node is not responsive (e.g. kubelet is not running, unreachable via network, etc.), such deletion may not be effective, since Kubernetes ensures that the Pod will not be deleted until it has confirmed that the container is fully removed and cleaned up on the node itself. If the JobConfig uses a concurrency policy like Forbid , this may indefinitely block future progress of the JobConfig from creating and starting new Jobs. As such, Furiko supports \"force deletion\" as a last resort, which forcefully deletes the Pod, allowing the Job to be terminated in order for the JobConfig to schedule new Jobs.","title":"Motivation"},{"location":"guide/execution/job/force-deletion/#deletion-process","text":"The \"force deletion\" process is analogous to simply running: kubectl delete pod <pod name> --grace-period=0 --force For more information, see Forced Pod termination on the Kubernetes documentation. Warning The official Kubernetes official documentation mentions the following warning about force deletion (emphasis added): IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion . If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency . Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately. Typically when running cron jobs, force deletion is safe most of the time, and in most cases would prefer availability of scheduling over strict guarantees. However, for jobs that are more crucial, they may choose to disable this functionality and instead trigger notifications to investigate node-level issues.","title":"Deletion Process"},{"location":"guide/execution/job/force-deletion/#configuration","text":"","title":"Configuration"},{"location":"guide/execution/job/force-deletion/#task-forbidtaskforcedeletion","text":"You can prevent force deletion by specifying forbidTaskForceDeletion: true in the JobTaskSpec, as follows: apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : template : # The normal task template taskTemplate : ... # Forbids force deletion of the task. forbidTaskForceDeletion : true In such a scenario, the controller will simply just wait until the Pod transitions into a terminated state before marking the Job as terminated as well.","title":"Task: forbidTaskForceDeletion"},{"location":"guide/execution/job/force-deletion/#global-forcedeletetasktimeoutseconds","text":"Specifies the default duration that the controller should wait after the deletionTimestamp of a task before forcefully deleting the task. Set to 0 to disable force deletion globally. For more information, see the Execution Dynamic Config documentation.","title":"Global: forceDeleteTaskTimeoutSeconds"},{"location":"guide/execution/job/garbage-collection/","text":"Garbage Collection As Furiko runs more and more Jobs in the cluster, they may tend to build up over time, even after they are finished. As such, Furiko imposes a time-to-live (TTL) on all Jobs after they are finished, so that the Kubernetes will not be too overloaded and controllers will be more responsive in general. Configuration Job: ttlSecondsAfterFinished Each Job can specify ttlSecondsAfterFinished which specifies the maximum TTL for a Job after it is finished, beyond which it will be deleted. If not specified, it will default to the global controller defaultTTLSecondsAfterFinished value. Global: defaultTTLSecondsAfterFinished The controller has a configuration value defaultTTLSecondsAfterFinished which specifies the default TTL for all Jobs created. To avoid issues with huge amounts of finished Jobs building up, a non-zero TTL is enforced. Defaults to 3600 (1 hour). Tip When running massive amounts of cron Jobs in the cluster, the API calls to Kubernetes tend to be extremely spiky and predictable every hour. In practice, we find that skewing the defaultTTLSecondsAfterFinished value by 3 minutes (i.e. setting it to 3780 ) would avoid making too many overlapping API calls to Kubernetes at the same time.","title":"Garbage Collection"},{"location":"guide/execution/job/garbage-collection/#garbage-collection","text":"As Furiko runs more and more Jobs in the cluster, they may tend to build up over time, even after they are finished. As such, Furiko imposes a time-to-live (TTL) on all Jobs after they are finished, so that the Kubernetes will not be too overloaded and controllers will be more responsive in general.","title":"Garbage Collection"},{"location":"guide/execution/job/garbage-collection/#configuration","text":"","title":"Configuration"},{"location":"guide/execution/job/garbage-collection/#job-ttlsecondsafterfinished","text":"Each Job can specify ttlSecondsAfterFinished which specifies the maximum TTL for a Job after it is finished, beyond which it will be deleted. If not specified, it will default to the global controller defaultTTLSecondsAfterFinished value.","title":"Job: ttlSecondsAfterFinished"},{"location":"guide/execution/job/garbage-collection/#global-defaultttlsecondsafterfinished","text":"The controller has a configuration value defaultTTLSecondsAfterFinished which specifies the default TTL for all Jobs created. To avoid issues with huge amounts of finished Jobs building up, a non-zero TTL is enforced. Defaults to 3600 (1 hour). Tip When running massive amounts of cron Jobs in the cluster, the API calls to Kubernetes tend to be extremely spiky and predictable every hour. In practice, we find that skewing the defaultTTLSecondsAfterFinished value by 3 minutes (i.e. setting it to 3780 ) would avoid making too many overlapping API calls to Kubernetes at the same time.","title":"Global: defaultTTLSecondsAfterFinished"},{"location":"guide/execution/job/killing-jobs/","text":"Killing Jobs A Job can be killed after it is started, either manually, through a timeout , or other means (such as parallel completion ). Using the CLI The easiest way to kill a Job execution is to use the furiko CLI tool : furiko kill <job-name> Kill Timestamp Each Job has a killTimestamp in its spec, which specifies the timestamp before the JobController tries to kill all of its active tasks. To kill a Job explicitly, set killTimestamp to the current time or earlier: apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : killTimestamp : \"2021-01-01T00:00:00Z\" You can also set a time in the future to kill the Job by setting a future time. The JobController will only start killing active tasks once killTimestamp is passed. Setting a kill timestamp does not imply that the Job will be terminated by time that the kill timestamp has been reached. It simply means that Automatic Termination A Job's task may be automatically terminated from several sources, including but not limited to: Task pending timeout Completion of other parallel tasks Termination Process The JobController uses deletion to kill tasks, and optionally reaches for force deletion as a last resort if the task is still not deleted after a timeout. Task Deletion The JobController uses normal resource deletion to kill a task. If the pod task executor is used, normal graceful termination will apply and container termination lifecycle hooks will be triggered. To adjust the grace period, use terminationGracePeriodSeconds . For more information, see Termination of Pods on the Kubernetes documentation. Force Deletion When a Pod is running on a Node that is no longer available, above methods to kill the Job will have no effect until the Node comes back up. As a last resort, the controller may opt to use force deletion to kill the task instead, which does not block any forward progress of the JobConfig. For more details and implications of using such a termination method, see the Force Deletion documentation.","title":"Killing Jobs"},{"location":"guide/execution/job/killing-jobs/#killing-jobs","text":"A Job can be killed after it is started, either manually, through a timeout , or other means (such as parallel completion ).","title":"Killing Jobs"},{"location":"guide/execution/job/killing-jobs/#using-the-cli","text":"The easiest way to kill a Job execution is to use the furiko CLI tool : furiko kill <job-name>","title":"Using the CLI"},{"location":"guide/execution/job/killing-jobs/#kill-timestamp","text":"Each Job has a killTimestamp in its spec, which specifies the timestamp before the JobController tries to kill all of its active tasks. To kill a Job explicitly, set killTimestamp to the current time or earlier: apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : killTimestamp : \"2021-01-01T00:00:00Z\" You can also set a time in the future to kill the Job by setting a future time. The JobController will only start killing active tasks once killTimestamp is passed. Setting a kill timestamp does not imply that the Job will be terminated by time that the kill timestamp has been reached. It simply means that","title":"Kill Timestamp"},{"location":"guide/execution/job/killing-jobs/#automatic-termination","text":"A Job's task may be automatically terminated from several sources, including but not limited to: Task pending timeout Completion of other parallel tasks","title":"Automatic Termination"},{"location":"guide/execution/job/killing-jobs/#termination-process","text":"The JobController uses deletion to kill tasks, and optionally reaches for force deletion as a last resort if the task is still not deleted after a timeout.","title":"Termination Process"},{"location":"guide/execution/job/killing-jobs/#task-deletion","text":"The JobController uses normal resource deletion to kill a task. If the pod task executor is used, normal graceful termination will apply and container termination lifecycle hooks will be triggered. To adjust the grace period, use terminationGracePeriodSeconds . For more information, see Termination of Pods on the Kubernetes documentation.","title":"Task Deletion"},{"location":"guide/execution/job/killing-jobs/#force-deletion","text":"When a Pod is running on a Node that is no longer available, above methods to kill the Job will have no effect until the Node comes back up. As a last resort, the controller may opt to use force deletion to kill the task instead, which does not block any forward progress of the JobConfig. For more details and implications of using such a termination method, see the Force Deletion documentation.","title":"Force Deletion"},{"location":"guide/execution/job/parallelism/","text":"Parallelism This page will discuss how to run multiple tasks in parallel for a single Job. Motivation A single Job without parallelism will have at most one task running at any time, which corresponds to a single Pod (if the pod task executor is used). Assume we have a JobConfig for processing all rows in a database, and is scheduled to run every day, which creates a Job (and thus a Pod) that will run to completion every day. Over time as the database grows in size, the Job may take longer to run since it now has more items to process, and may eventually take too long for business requirements. Apart from vertically scaling the Pod, you may wish to parallelize the job processing into multiple partitions, which can be achieved by specifying the number of partitions via withCount . Other means of expanding a job into multiple parallel tasks include one parallel task for each item in a list ( withKeys ), or for each combination of values in a matrix ( withMatrix ). The equivalent feature in Kubernetes is Indexed Jobs . Sample Configuration withCount withKeys withMatrix apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withCount : 3 completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image args : - ./bin/consume -shard-index=${task.index_num} apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withKeys : - golang - python - nodejs completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image args : - ./bin/run -lang=${task.index_key} apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withMatrix : goos : - linux - darwin goarch : - amd64 - arm64 completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image env : - name : GOOS value : ${task.index_matrix.goos} - name : GOARCH value : ${task.index_matrix.goarch} args : - go build ./... Configuration Options Exactly one of withCount , withKeys , or withMatrix must be specified. withCount Specifies the number of parallel tasks to be executed in parallel. Must be a positive integer. If withCount is used, then the task.index_num context variable will be available to substitute the index number (from 0 to N-1) as a context variable . withKeys Specifies keys that will correspond to each parallel task. If withKeys is used, then the task.index_key context variable will be available to substitute the index key as a context variable . Each key must be a non-empty string. withMatrix Specifies a matrix of key-value pairs, such that each matrix combination will correspond to each parallel task. For example, given the following configuration: withMatrix : goos : - linux - darwin - windows goarch : - amd64 - arm64 A total of 6 tasks will be run in parallel: goos goarch linux amd64 linux arm64 darwin amd64 darwin arm64 windows amd64 windows arm64 If withMatrix is used, then the task.index_matrix.<key> context variable will be available to substitute the index key as a context variable . Each key must be a non-empty string and contain only lowercase letters, numbers, underscores, and dashes. Each value must be a non-empty string. completionStrategy Determines when the Job is complete when multiple tasks are running in parallel. Must be one of the following values. Name Description AllSuccessful Completed only once each parallel index has succeeded. AnySuccessful Completed when any parallel index has succeeeded. AllSuccessful This strategy means that the Job is completed only once each parallel index has succeeded, and the completion strategy is deemed to have succeeded. If any parallel index has exhausted all its retries (i.e. it has failed), then the completion strategy is deemed to have failed. All remaining tasks will be terminated. AnySuccessful This strategy means that the Job is completed when any parallel index has succeeeded, and the completion strategy is deemed to have succeeded. All remaining tasks will be terminated. If all parallel indexes have exhausted all of their retries (i.e. it has failed), only then will the completion strategy be deemed to have failed.","title":"Parallelism"},{"location":"guide/execution/job/parallelism/#parallelism","text":"This page will discuss how to run multiple tasks in parallel for a single Job.","title":"Parallelism"},{"location":"guide/execution/job/parallelism/#motivation","text":"A single Job without parallelism will have at most one task running at any time, which corresponds to a single Pod (if the pod task executor is used). Assume we have a JobConfig for processing all rows in a database, and is scheduled to run every day, which creates a Job (and thus a Pod) that will run to completion every day. Over time as the database grows in size, the Job may take longer to run since it now has more items to process, and may eventually take too long for business requirements. Apart from vertically scaling the Pod, you may wish to parallelize the job processing into multiple partitions, which can be achieved by specifying the number of partitions via withCount . Other means of expanding a job into multiple parallel tasks include one parallel task for each item in a list ( withKeys ), or for each combination of values in a matrix ( withMatrix ). The equivalent feature in Kubernetes is Indexed Jobs .","title":"Motivation"},{"location":"guide/execution/job/parallelism/#sample-configuration","text":"withCount withKeys withMatrix apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withCount : 3 completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image args : - ./bin/consume -shard-index=${task.index_num} apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withKeys : - golang - python - nodejs completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image args : - ./bin/run -lang=${task.index_key} apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : parallelism : withMatrix : goos : - linux - darwin goarch : - amd64 - arm64 completionStrategy : AllSuccessful template : taskTemplate : pod : spec : containers : - name : job-container image : my-image env : - name : GOOS value : ${task.index_matrix.goos} - name : GOARCH value : ${task.index_matrix.goarch} args : - go build ./...","title":"Sample Configuration"},{"location":"guide/execution/job/parallelism/#configuration-options","text":"Exactly one of withCount , withKeys , or withMatrix must be specified.","title":"Configuration Options"},{"location":"guide/execution/job/parallelism/#withcount","text":"Specifies the number of parallel tasks to be executed in parallel. Must be a positive integer. If withCount is used, then the task.index_num context variable will be available to substitute the index number (from 0 to N-1) as a context variable .","title":"withCount"},{"location":"guide/execution/job/parallelism/#withkeys","text":"Specifies keys that will correspond to each parallel task. If withKeys is used, then the task.index_key context variable will be available to substitute the index key as a context variable . Each key must be a non-empty string.","title":"withKeys"},{"location":"guide/execution/job/parallelism/#withmatrix","text":"Specifies a matrix of key-value pairs, such that each matrix combination will correspond to each parallel task. For example, given the following configuration: withMatrix : goos : - linux - darwin - windows goarch : - amd64 - arm64 A total of 6 tasks will be run in parallel: goos goarch linux amd64 linux arm64 darwin amd64 darwin arm64 windows amd64 windows arm64 If withMatrix is used, then the task.index_matrix.<key> context variable will be available to substitute the index key as a context variable . Each key must be a non-empty string and contain only lowercase letters, numbers, underscores, and dashes. Each value must be a non-empty string.","title":"withMatrix"},{"location":"guide/execution/job/parallelism/#completionstrategy","text":"Determines when the Job is complete when multiple tasks are running in parallel. Must be one of the following values. Name Description AllSuccessful Completed only once each parallel index has succeeded. AnySuccessful Completed when any parallel index has succeeeded.","title":"completionStrategy"},{"location":"guide/execution/job/parallelism/#allsuccessful","text":"This strategy means that the Job is completed only once each parallel index has succeeded, and the completion strategy is deemed to have succeeded. If any parallel index has exhausted all its retries (i.e. it has failed), then the completion strategy is deemed to have failed. All remaining tasks will be terminated.","title":"AllSuccessful"},{"location":"guide/execution/job/parallelism/#anysuccessful","text":"This strategy means that the Job is completed when any parallel index has succeeeded, and the completion strategy is deemed to have succeeded. All remaining tasks will be terminated. If all parallel indexes have exhausted all of their retries (i.e. it has failed), only then will the completion strategy be deemed to have failed.","title":"AnySuccessful"},{"location":"guide/execution/job/sample-configuration/","text":"Sample Configuration This page contains sample configurations for creating a Job. From JobConfig Most of the time, you should be creating Jobs from a JobConfig as follows, as described in Adhoc Execution . apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : # Define the JobConfig (in the same namespace) to use as the template for the Job. configName : jobconfig-sample # YAML or JSON encoded string of option values to pass to the JobConfig. # Will be evaluated and added to substitutions. optionValues : |- username: Example User image-tag: 3.15 # Optionally specify conditions for when the Job can start. startPolicy : # Defines the behavior for multiple concurrent Jobs. # If not specified, will default to the JobConfig's concurrency.policy. concurrencyPolicy : Enqueue # Start only after this time. startAfter : 2022-03-06T00:27:00+08:00 Standalone Job You can also create a Job independent of a JobConfig, as described here . You must specify the template yourself. apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : # Specify the type of Job. type : Adhoc # Map of key-value pairs to substitute into the task template. substitutions : option.username : Example User # Describes how to create the Job. template : # Specifies how to run the job in parallel. parallelism : # Run 3 tasks in parallel. withCount : 3 # Wait for all 3 tasks to succeed before deemed as successful. completionStrategy : AllSuccessful # Specifies maximum number of attempts for each task, defaults to 1. maxAttempts : 3 # Optional delay between each task retry. retryDelaySeconds : 10 # Optional duration in seconds for how long each task should be pending for # until it gets killed. taskPendingTimeoutSeconds : 1800 # Forbids force deletion of tasks. forbidTaskForceDeletion : true # The template for each task to be created by the Job. taskTemplate : # Specify how to create the task as a Pod. This is just a PodTemplateSpec. pod : spec : containers : - args : - echo - \"Hello world, ${option.username}!\" env : - name : JOBCONFIG_NAME value : jobconfig-sample - name : JOB_NAME value : ${job.name} - name : TASK_NAME value : ${task.name} - name : TASK_INDEX value : ${task.index_num} image : alpine name : job-container resources : limits : cpu : 100m memory : 64Mi # Optional duration that the Job should live after it is finished. ttlSecondsAfterFinished : 3600","title":"Sample Configuration"},{"location":"guide/execution/job/sample-configuration/#sample-configuration","text":"This page contains sample configurations for creating a Job.","title":"Sample Configuration"},{"location":"guide/execution/job/sample-configuration/#from-jobconfig","text":"Most of the time, you should be creating Jobs from a JobConfig as follows, as described in Adhoc Execution . apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : # Define the JobConfig (in the same namespace) to use as the template for the Job. configName : jobconfig-sample # YAML or JSON encoded string of option values to pass to the JobConfig. # Will be evaluated and added to substitutions. optionValues : |- username: Example User image-tag: 3.15 # Optionally specify conditions for when the Job can start. startPolicy : # Defines the behavior for multiple concurrent Jobs. # If not specified, will default to the JobConfig's concurrency.policy. concurrencyPolicy : Enqueue # Start only after this time. startAfter : 2022-03-06T00:27:00+08:00","title":"From JobConfig"},{"location":"guide/execution/job/sample-configuration/#standalone-job","text":"You can also create a Job independent of a JobConfig, as described here . You must specify the template yourself. apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : # Specify the type of Job. type : Adhoc # Map of key-value pairs to substitute into the task template. substitutions : option.username : Example User # Describes how to create the Job. template : # Specifies how to run the job in parallel. parallelism : # Run 3 tasks in parallel. withCount : 3 # Wait for all 3 tasks to succeed before deemed as successful. completionStrategy : AllSuccessful # Specifies maximum number of attempts for each task, defaults to 1. maxAttempts : 3 # Optional delay between each task retry. retryDelaySeconds : 10 # Optional duration in seconds for how long each task should be pending for # until it gets killed. taskPendingTimeoutSeconds : 1800 # Forbids force deletion of tasks. forbidTaskForceDeletion : true # The template for each task to be created by the Job. taskTemplate : # Specify how to create the task as a Pod. This is just a PodTemplateSpec. pod : spec : containers : - args : - echo - \"Hello world, ${option.username}!\" env : - name : JOBCONFIG_NAME value : jobconfig-sample - name : JOB_NAME value : ${job.name} - name : TASK_NAME value : ${task.name} - name : TASK_INDEX value : ${task.index_num} image : alpine name : job-container resources : limits : cpu : 100m memory : 64Mi # Optional duration that the Job should live after it is finished. ttlSecondsAfterFinished : 3600","title":"Standalone Job"},{"location":"guide/execution/job/start-policy/","text":"Start Policy A Job may have an optional Start Policy , which defines certain preconditions that must be met before the Job can be started. Sample Configuration apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : startPolicy : concurrencyPolicy : Enqueue startAfter : 2022-03-06T00:27:00+08:00 Configuration Options concurrencyPolicy Specifies the behavior when there are other concurrent Jobs for the same JobConfig. The set of allowed values are identical to those specified in a JobConfig's concurrency.policy (see here ). If not specified, it will be inherited from the JobConfig's concurrency.policy value. startAfter Specifies an optional timestamp that the Job can only be started after. This allows you to schedule a Job to be only run at a later point in time. Info The behavior when the Job starts will still be subject to the value of concurrencyPolicy , which is inherited from the JobConfig's concurrency value if not specified. This means that if Forbid is specified, the Job may be queued but eventually fail with AdmissionError . In many cases, the Enqueue policy is usually desired when using startAfter .","title":"Start Policy"},{"location":"guide/execution/job/start-policy/#start-policy","text":"A Job may have an optional Start Policy , which defines certain preconditions that must be met before the Job can be started.","title":"Start Policy"},{"location":"guide/execution/job/start-policy/#sample-configuration","text":"apiVersion : execution.furiko.io/v1alpha1 kind : Job spec : startPolicy : concurrencyPolicy : Enqueue startAfter : 2022-03-06T00:27:00+08:00","title":"Sample Configuration"},{"location":"guide/execution/job/start-policy/#configuration-options","text":"","title":"Configuration Options"},{"location":"guide/execution/job/start-policy/#concurrencypolicy","text":"Specifies the behavior when there are other concurrent Jobs for the same JobConfig. The set of allowed values are identical to those specified in a JobConfig's concurrency.policy (see here ). If not specified, it will be inherited from the JobConfig's concurrency.policy value.","title":"concurrencyPolicy"},{"location":"guide/execution/job/start-policy/#startafter","text":"Specifies an optional timestamp that the Job can only be started after. This allows you to schedule a Job to be only run at a later point in time. Info The behavior when the Job starts will still be subject to the value of concurrencyPolicy , which is inherited from the JobConfig's concurrency value if not specified. This means that if Forbid is specified, the Job may be queued but eventually fail with AdmissionError . In many cases, the Enqueue policy is usually desired when using startAfter .","title":"startAfter"},{"location":"guide/execution/job/task-executor/","text":"Task Executors Furiko supports an extensible task executor interface, which allows tasks to be created, managed and reconciled in the same way regardless of the actual backing object. Task Index A Job creates one or more tasks during its lifecycle. Each task corresponds to a single parallel and retry index for the Job: Parallel Index : If the Job has N parallel tasks , then there are N parallel indexes. Otherwise, there is 1 parallel index (i.e. it is not parallel). Retry Index : If the Job specifies maxAttempts of M, then there are up to M retry indexes. Therefore, each Task corresponds to a single (N, M) combination of the above indexes. As an example, for a Job with a parallelism factor of 3 and maxAttempts of 2 , up to 6 tasks will be created for the Job (assuming that each attempt had failed). Task Executor List Currently the only task executor available in Furiko is pod , but more task executors are planned in the Roadmap . pod Each task will be created as a Pod . On a typical Kubernetes cluster, this translates to one or more containers that run to completion. Example pod TaskTemplate taskTemplate : pod : metadata : annotations : app.kubernetes.io/name : jobconfig-sample-pod spec : containers : - args : - echo - Hello world! image : alpine name : job-container resources : limits : cpu : 100m memory : 64Mi Virtual Pods Using tools such as Virtual Kubelet , it may be possible for the Pods to manifest as objects other than a CRI container in Kubernetes. If Virtual Kubelet is provisioned in the cluster, some possible use cases include: Running Pods on serverless compute platforms like AWS Fargate Extending to multi-cluster with Admiralty The usage and support of Virtual Kubelet is outside the scope of the Furiko project. argoWorkflow Info Currently planned in the Roadmap . Each task will be created as an Argo Workflow object . Requires Argo Workflows to be installed in the cluster (not included with Furiko).","title":"Task Executors"},{"location":"guide/execution/job/task-executor/#task-executors","text":"Furiko supports an extensible task executor interface, which allows tasks to be created, managed and reconciled in the same way regardless of the actual backing object.","title":"Task Executors"},{"location":"guide/execution/job/task-executor/#task-index","text":"A Job creates one or more tasks during its lifecycle. Each task corresponds to a single parallel and retry index for the Job: Parallel Index : If the Job has N parallel tasks , then there are N parallel indexes. Otherwise, there is 1 parallel index (i.e. it is not parallel). Retry Index : If the Job specifies maxAttempts of M, then there are up to M retry indexes. Therefore, each Task corresponds to a single (N, M) combination of the above indexes. As an example, for a Job with a parallelism factor of 3 and maxAttempts of 2 , up to 6 tasks will be created for the Job (assuming that each attempt had failed).","title":"Task Index"},{"location":"guide/execution/job/task-executor/#task-executor-list","text":"Currently the only task executor available in Furiko is pod , but more task executors are planned in the Roadmap .","title":"Task Executor List"},{"location":"guide/execution/job/task-executor/#pod","text":"Each task will be created as a Pod . On a typical Kubernetes cluster, this translates to one or more containers that run to completion. Example pod TaskTemplate taskTemplate : pod : metadata : annotations : app.kubernetes.io/name : jobconfig-sample-pod spec : containers : - args : - echo - Hello world! image : alpine name : job-container resources : limits : cpu : 100m memory : 64Mi","title":"pod"},{"location":"guide/execution/job/task-executor/#virtual-pods","text":"Using tools such as Virtual Kubelet , it may be possible for the Pods to manifest as objects other than a CRI container in Kubernetes. If Virtual Kubelet is provisioned in the cluster, some possible use cases include: Running Pods on serverless compute platforms like AWS Fargate Extending to multi-cluster with Admiralty The usage and support of Virtual Kubelet is outside the scope of the Furiko project.","title":"Virtual Pods"},{"location":"guide/execution/job/task-executor/#argoworkflow","text":"Info Currently planned in the Roadmap . Each task will be created as an Argo Workflow object . Requires Argo Workflows to be installed in the cluster (not included with Furiko).","title":"argoWorkflow"},{"location":"guide/execution/job/timeout-retries/","text":"Timeouts and Retries Furiko provides several mechanisms to impose timeouts and retrying of failed Jobs. Task-level Timeouts The following timeouts apply to individual tasks in a Job. pendingTimeoutSeconds Specifies the maximum duration that a single task can be pending for. This includes the time taken for scheduling, image pull and container creation. If not specified, defaults to the global controller defaultTaskPendingTimeoutSeconds value. Info If the Pod cannot pull the container image, it will remain in ImagePullBackOff indefinitely. A pending timeout helps to stop these Jobs eventually. runningTimeoutSeconds Specifies the maximum duration that a single task can be running for. The time starts once the task starts running. If not specified, defaults to no timeout. Tip It is recommended to use timeout(1) available on Unix systems, which provides several additional mechanisms to control the exit code and signals being sent on timeout. Another way is to also use timeouts in your application directly, and read the value from a Job Option . Todo This timeout is not yet implemented. taskTemplate.pod.spec.activeDeadlineSeconds You can also set the Pod's activeDeadlineSeconds directly, which is the duration relative to the Pod's startTime before Kubelet will actively try to kill associated containers. Job-level Timeouts The following timeouts apply to the entire Job across all tasks. jobTimeoutSeconds Specifies a global timeout for the entire Job across all tasks. The time starts when the Job is started, and is inclusive of the retry delay and time spent waiting for the tasks to start running. Todo This timeout is not yet implemented. Retries Furiko retries failed Jobs by creating a new Task. If a Node is misconfigured or has some host-level issue, using restartPolicy: OnFailure to recreate the container would not be sufficient to avoid spurious Job failures which may only be resolved by running the Task on a different Node. As such, Furiko recommends using Job-level retries, which recreates an entirely new Pod. maxAttempts Specifies the maximum number of task attempts. If the job is a parallel job , this corresponds to the maximum number of attempts for each parallel index. If not specified, defaults to 1 (i.e. no retries). Must be a positive integer. retryDelaySeconds Specifies the duration between the last failed task and creation of the next task. If the job is a parallel job , the retry delay is from the time of the last failed task with the same parallel index. That is, if there are two parallel tasks - index 0 and index 1 - which failed at t=0 and t=15 , with retryDelaySeconds of 30 , the controller will only create the next attempts at t=30 and t=45 respectively. If not specified, it means there is no delay between creating task attempts. restartPolicy If the Job uses both restartPolicy: OnFailure in conjunction with Furiko Job-level tries, Jobs may take a longer time before finally terminating in failure. If a Job is in a CrashLoopBackOff , it will be deemed to be still \"pending\", and if it remains/transitions to this state even after its pending timeout , it will be killed. The next Task will be only created after retryDelaySeconds , which results in the creation of a brand-new Pod. This means that the total time taken before terminating in failure would be roughly around pendingTimeoutSeconds * maxAttempts + retryDelaySeconds * (maxAttempts - 1) , rather than simply the sum of all tasks' running durations.","title":"Timeouts and Retries"},{"location":"guide/execution/job/timeout-retries/#timeouts-and-retries","text":"Furiko provides several mechanisms to impose timeouts and retrying of failed Jobs.","title":"Timeouts and Retries"},{"location":"guide/execution/job/timeout-retries/#task-level-timeouts","text":"The following timeouts apply to individual tasks in a Job.","title":"Task-level Timeouts"},{"location":"guide/execution/job/timeout-retries/#pendingtimeoutseconds","text":"Specifies the maximum duration that a single task can be pending for. This includes the time taken for scheduling, image pull and container creation. If not specified, defaults to the global controller defaultTaskPendingTimeoutSeconds value. Info If the Pod cannot pull the container image, it will remain in ImagePullBackOff indefinitely. A pending timeout helps to stop these Jobs eventually.","title":"pendingTimeoutSeconds"},{"location":"guide/execution/job/timeout-retries/#runningtimeoutseconds","text":"Specifies the maximum duration that a single task can be running for. The time starts once the task starts running. If not specified, defaults to no timeout. Tip It is recommended to use timeout(1) available on Unix systems, which provides several additional mechanisms to control the exit code and signals being sent on timeout. Another way is to also use timeouts in your application directly, and read the value from a Job Option . Todo This timeout is not yet implemented.","title":"runningTimeoutSeconds"},{"location":"guide/execution/job/timeout-retries/#tasktemplatepodspecactivedeadlineseconds","text":"You can also set the Pod's activeDeadlineSeconds directly, which is the duration relative to the Pod's startTime before Kubelet will actively try to kill associated containers.","title":"taskTemplate.pod.spec.activeDeadlineSeconds"},{"location":"guide/execution/job/timeout-retries/#job-level-timeouts","text":"The following timeouts apply to the entire Job across all tasks.","title":"Job-level Timeouts"},{"location":"guide/execution/job/timeout-retries/#jobtimeoutseconds","text":"Specifies a global timeout for the entire Job across all tasks. The time starts when the Job is started, and is inclusive of the retry delay and time spent waiting for the tasks to start running. Todo This timeout is not yet implemented.","title":"jobTimeoutSeconds"},{"location":"guide/execution/job/timeout-retries/#retries","text":"Furiko retries failed Jobs by creating a new Task. If a Node is misconfigured or has some host-level issue, using restartPolicy: OnFailure to recreate the container would not be sufficient to avoid spurious Job failures which may only be resolved by running the Task on a different Node. As such, Furiko recommends using Job-level retries, which recreates an entirely new Pod.","title":"Retries"},{"location":"guide/execution/job/timeout-retries/#maxattempts","text":"Specifies the maximum number of task attempts. If the job is a parallel job , this corresponds to the maximum number of attempts for each parallel index. If not specified, defaults to 1 (i.e. no retries). Must be a positive integer.","title":"maxAttempts"},{"location":"guide/execution/job/timeout-retries/#retrydelayseconds","text":"Specifies the duration between the last failed task and creation of the next task. If the job is a parallel job , the retry delay is from the time of the last failed task with the same parallel index. That is, if there are two parallel tasks - index 0 and index 1 - which failed at t=0 and t=15 , with retryDelaySeconds of 30 , the controller will only create the next attempts at t=30 and t=45 respectively. If not specified, it means there is no delay between creating task attempts.","title":"retryDelaySeconds"},{"location":"guide/execution/job/timeout-retries/#restartpolicy","text":"If the Job uses both restartPolicy: OnFailure in conjunction with Furiko Job-level tries, Jobs may take a longer time before finally terminating in failure. If a Job is in a CrashLoopBackOff , it will be deemed to be still \"pending\", and if it remains/transitions to this state even after its pending timeout , it will be killed. The next Task will be only created after retryDelaySeconds , which results in the creation of a brand-new Pod. This means that the total time taken before terminating in failure would be roughly around pendingTimeoutSeconds * maxAttempts + retryDelaySeconds * (maxAttempts - 1) , rather than simply the sum of all tasks' running durations.","title":"restartPolicy"},{"location":"guide/execution/jobconfig/","text":"JobConfig This page describes how to configure a JobConfig. You can find a full sample configuration here . Template Every JobConfig needs a template which describes the Job to be created, known as the JobTemplate . For more information, refer to the Job section of the docs. Scheduling You can configure a JobConfig to be periodically scheduled, using an extended Cron syntax to specify the schedule, an optional timezone, as well as a concurrency policy to define the behavior when there are multiple concurrent jobs. For more information, see Scheduling . Concurrency You can define the expected behavior for executing multiple Jobs that belong the same JobConfig. For more information, see Concurrency . Job Options You also can parameterize the JobConfig which describes what arguments it can accept (according to various types), and how they should be substituted into the Task's template. For example, if we wanted to parameterize an input flag to a script, say --config-name , we could define a job option which substitutes the task's args field as follows: Example TaskTemplate taskTemplate : pod : spec : containers : - name : job-container args : - python - my_script.py - \"--config-name=${option.config_name}\" Subsequently, the job option config_name can be configured to support either free text or selected from a list of options. This structured job option configuration allows various tools to understand how to display option inputs to the user. For more information, see Job Options .","title":"JobConfig"},{"location":"guide/execution/jobconfig/#jobconfig","text":"This page describes how to configure a JobConfig. You can find a full sample configuration here .","title":"JobConfig"},{"location":"guide/execution/jobconfig/#template","text":"Every JobConfig needs a template which describes the Job to be created, known as the JobTemplate . For more information, refer to the Job section of the docs.","title":"Template"},{"location":"guide/execution/jobconfig/#scheduling","text":"You can configure a JobConfig to be periodically scheduled, using an extended Cron syntax to specify the schedule, an optional timezone, as well as a concurrency policy to define the behavior when there are multiple concurrent jobs. For more information, see Scheduling .","title":"Scheduling"},{"location":"guide/execution/jobconfig/#concurrency","text":"You can define the expected behavior for executing multiple Jobs that belong the same JobConfig. For more information, see Concurrency .","title":"Concurrency"},{"location":"guide/execution/jobconfig/#job-options","text":"You also can parameterize the JobConfig which describes what arguments it can accept (according to various types), and how they should be substituted into the Task's template. For example, if we wanted to parameterize an input flag to a script, say --config-name , we could define a job option which substitutes the task's args field as follows: Example TaskTemplate taskTemplate : pod : spec : containers : - name : job-container args : - python - my_script.py - \"--config-name=${option.config_name}\" Subsequently, the job option config_name can be configured to support either free text or selected from a list of options. This structured job option configuration allows various tools to understand how to display option inputs to the user. For more information, see Job Options .","title":"Job Options"},{"location":"guide/execution/jobconfig/concurrency/","text":"Concurrency The concurrency of a JobConfig defines the behavior when a single JobConfig has multiple concurrent Jobs. For example, the cron.expression may schedule another Job while the previous Job is active, or an ad-hoc Job may be created while another Job is also active. The concurrency.policy defines what to do in such a scenario. Sample Configuration apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig spec : concurrency : policy : Forbid Configuration Options policy The ConcurrencyPolicy defines the behavior when a single JobConfig has multiple concurrent Jobs. When specified in a JobConfig, this controls the behavior of automatically scheduled Jobs and when Jobs are created on an ad-hoc basis (e.g. via kubectl create ). It is also specified as part of a Job's startPolicy , which inherits the policy from the JobConfig, but could be overridden if so desired. Policy Description Allow Places no restriction on the number of Jobs that can be started concurrently whatsoever. Forbid Forbids multiple Jobs to be started concurrently, and will be dropped/rejected immediately. Enqueue Places the incoming Job into a queue to be started once all other Jobs have finished. Policy Notes Using Allow could potentially quickly result in resource exhaustion of the cluster for a misconfigured cron.expression that runs too frequently. When using Enqueue , Jobs are started in order of their creationTimestamp . Can also be used in conjunction with startAfter . Furiko currently does not support a maximum queue length for Enqueue , which could result in a huge backlog of Jobs to be started. Recommendations In practice, we recommend using Forbid as a sane default for most jobs for the following reasons: Many automation jobs are not concurrent-safe, and risk performing duplicate operations or running into TOCTTOU bugs when run concurrently. In an offline processing scenario, even if the previously running Job runs longer than normal, it would still be able to process all work to be done that would have been picked up by the later scheduled Job anyway. One pattern that is commonly adopted is to: Use Forbid for policy . Specify a cron.expression which defines the maximum duration between any two Jobs being executed. This roughly equates to the maximum duration for an offline processing queue latency. Preventing Concurrent Jobs The following scenarios explain what happens when there is another concurrent Job when using the Forbid policy: Another Job is about to be scheduled by the CronController: Will be silently dropped and from scheduling. An ad-hoc Job is created (e.g. using kubectl ): Will be rejected by the validating admission webhook in most cases. If the Job was created almost at the same time that the concurrent Job started, it may be possible for it to bypass the validation webhook. In this case, the JobQueueController will reject the Job with AdmissionError like as follows: $ kubectl describe kjob jobconfig-sample-btzqx ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning AdmissionRefused 12s JobQueueController Cannot start new Job, jobconfig-sample has 1 active Jobs but concurrency policy is Forbid Warning Failed 12s JobController Job failed with result: AdmissionError Warning While Furiko tries its best to prevent concurrent Jobs, it is not a replacement for a distributed lock in the application. For highly critical operations, it is still recommended for Job developers to use traditional distributed locks or database systems (e.g. jobs processing money or transactions).","title":"Concurrency"},{"location":"guide/execution/jobconfig/concurrency/#concurrency","text":"The concurrency of a JobConfig defines the behavior when a single JobConfig has multiple concurrent Jobs. For example, the cron.expression may schedule another Job while the previous Job is active, or an ad-hoc Job may be created while another Job is also active. The concurrency.policy defines what to do in such a scenario.","title":"Concurrency"},{"location":"guide/execution/jobconfig/concurrency/#sample-configuration","text":"apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig spec : concurrency : policy : Forbid","title":"Sample Configuration"},{"location":"guide/execution/jobconfig/concurrency/#configuration-options","text":"","title":"Configuration Options"},{"location":"guide/execution/jobconfig/concurrency/#policy","text":"The ConcurrencyPolicy defines the behavior when a single JobConfig has multiple concurrent Jobs. When specified in a JobConfig, this controls the behavior of automatically scheduled Jobs and when Jobs are created on an ad-hoc basis (e.g. via kubectl create ). It is also specified as part of a Job's startPolicy , which inherits the policy from the JobConfig, but could be overridden if so desired. Policy Description Allow Places no restriction on the number of Jobs that can be started concurrently whatsoever. Forbid Forbids multiple Jobs to be started concurrently, and will be dropped/rejected immediately. Enqueue Places the incoming Job into a queue to be started once all other Jobs have finished.","title":"policy"},{"location":"guide/execution/jobconfig/concurrency/#policy-notes","text":"Using Allow could potentially quickly result in resource exhaustion of the cluster for a misconfigured cron.expression that runs too frequently. When using Enqueue , Jobs are started in order of their creationTimestamp . Can also be used in conjunction with startAfter . Furiko currently does not support a maximum queue length for Enqueue , which could result in a huge backlog of Jobs to be started.","title":"Policy Notes"},{"location":"guide/execution/jobconfig/concurrency/#recommendations","text":"In practice, we recommend using Forbid as a sane default for most jobs for the following reasons: Many automation jobs are not concurrent-safe, and risk performing duplicate operations or running into TOCTTOU bugs when run concurrently. In an offline processing scenario, even if the previously running Job runs longer than normal, it would still be able to process all work to be done that would have been picked up by the later scheduled Job anyway. One pattern that is commonly adopted is to: Use Forbid for policy . Specify a cron.expression which defines the maximum duration between any two Jobs being executed. This roughly equates to the maximum duration for an offline processing queue latency.","title":"Recommendations"},{"location":"guide/execution/jobconfig/concurrency/#preventing-concurrent-jobs","text":"The following scenarios explain what happens when there is another concurrent Job when using the Forbid policy: Another Job is about to be scheduled by the CronController: Will be silently dropped and from scheduling. An ad-hoc Job is created (e.g. using kubectl ): Will be rejected by the validating admission webhook in most cases. If the Job was created almost at the same time that the concurrent Job started, it may be possible for it to bypass the validation webhook. In this case, the JobQueueController will reject the Job with AdmissionError like as follows: $ kubectl describe kjob jobconfig-sample-btzqx ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning AdmissionRefused 12s JobQueueController Cannot start new Job, jobconfig-sample has 1 active Jobs but concurrency policy is Forbid Warning Failed 12s JobController Job failed with result: AdmissionError Warning While Furiko tries its best to prevent concurrent Jobs, it is not a replacement for a distributed lock in the application. For highly critical operations, it is still recommended for Job developers to use traditional distributed locks or database systems (e.g. jobs processing money or transactions).","title":"Preventing Concurrent Jobs"},{"location":"guide/execution/jobconfig/context-variables/","text":"Context Variables This page introduces the concept of Context Variables , which allows Furiko users to parameterize their jobs based on certain variables. Overview Introduction A variable looks like ${job.name} . This context variable uses the context job , with the variable name name . Users who are familiar with Rundeck may find this concept familiar, as it was inspired by the corresponding feature in Rundeck: https://docs.rundeck.com/docs/manual/job-workflows.html#context-variables Usage Context variables can be used inside the Job/JobConfig's task template, like so: Example TaskTemplate taskTemplate : pod : spec : containers : - name : job-container args : - echo - \"Job Name: ${job.name}\" For pod , only the following fields support context variable substitution: .spec.containers.*.image .spec.containers.*.command.* .spec.containers.*.args.* .spec.containers.*.env.*.value .spec.initContainers.*.image .spec.initContainers.*.command.* .spec.initContainers.*.args.* .spec.initContainers.*.env.*.value Variables will be substituted as a string as-is in the fields that they are defined in. If using in the command or args fields, you may need to escape them with double quotes (e.g. \"${job.name}\" ). Contexts Each context corresponds to a stage in the Job's lifecycle when certain variables are made available. The following is a list of all contexts and available variables. jobconfig Variables are evaluated when creating a Job from a JobConfig, such as during cron scheduling or ad-hoc starts of a Job. Variable Description Example jobconfig.name Name of the JobConfig that created the Job. my-example-jobconfig jobconfig.namespace Namespace of the JobConfig that created the Job. furiko jobconfig.uid UID of the JobConfig that created the Job. 4247b21e-713f-46db-b8f5-39917893577e jobconfig.cron_schedule Cron expression that was used to schedule the Job. H/15 * * * * jobconfig.cron_timezone Cron timezone that was used to schedule the Job. Asia/Singapore job Variables are evaluated when creating a task for a Job. This takes place after the Job is started. Variable Description Example job.name Name of the Job that created the task. my-example-jobconfig.1646586480 job.namespace Namespace of the Job that created the task. furiko job.type Type of Job, e.g. Adhoc , Scheduled . Adhoc task Variables are evaluated when creating a task. Variable Description Example task.name Name of the task. my-example-jobconfig.1646586480.1 task.namespace Namespace of the task. furiko task.retry_index The retry index of the task, starts from 1. 1 Additionally, the following variables are included when a parallel job is created: Variable Description Example task.index_num If withCount is used, the task index number (from 0...N-1). 2 task.index_key If withKeys is used, the task index key (as a string). item-3 task.index_matrix.<key> If withMatrix is used, the value of the corresponding key in the matrix for this task index. value-1 option The option context is a special context that allows for user-defined inputs to parameterize the Job. For more information, see Job Options . Substitutions Variables are evaluated based on their context, but substitution is only performed only when creating the task object (i.e. when creating the PodSpec ). Because of this, the Job exposes a substitutions field which captures all evaluated substitutions so far before finally evaluating it at the end. Users may also override certain context variables by manually specifying it in the substitutions field when creating the Job. They will take precedence over all other variables that are evaluated (context variables and option variables). For example, given the following configuration below: Example: Custom Substitutions apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : dummy-sample- spec : configName : jobconfig-sample substitutions : option.username : \"mary\" jobconfig.name : \"dummy-sample\" optionValues : |- username: \"bob\" expiry: \"168h\" option.username will be evaluated to mary ( substitutions takes precedence) option.expiry will be evaluated to 168h (from optionValues ) jobconfig.name will be evaluated to dummy-sample ( substitutions takes precedence) jobconfig.namespace will be evaluated to default (from the jobconfig context)","title":"Context Variables"},{"location":"guide/execution/jobconfig/context-variables/#context-variables","text":"This page introduces the concept of Context Variables , which allows Furiko users to parameterize their jobs based on certain variables.","title":"Context Variables"},{"location":"guide/execution/jobconfig/context-variables/#overview","text":"","title":"Overview"},{"location":"guide/execution/jobconfig/context-variables/#introduction","text":"A variable looks like ${job.name} . This context variable uses the context job , with the variable name name . Users who are familiar with Rundeck may find this concept familiar, as it was inspired by the corresponding feature in Rundeck: https://docs.rundeck.com/docs/manual/job-workflows.html#context-variables","title":"Introduction"},{"location":"guide/execution/jobconfig/context-variables/#usage","text":"Context variables can be used inside the Job/JobConfig's task template, like so: Example TaskTemplate taskTemplate : pod : spec : containers : - name : job-container args : - echo - \"Job Name: ${job.name}\" For pod , only the following fields support context variable substitution: .spec.containers.*.image .spec.containers.*.command.* .spec.containers.*.args.* .spec.containers.*.env.*.value .spec.initContainers.*.image .spec.initContainers.*.command.* .spec.initContainers.*.args.* .spec.initContainers.*.env.*.value Variables will be substituted as a string as-is in the fields that they are defined in. If using in the command or args fields, you may need to escape them with double quotes (e.g. \"${job.name}\" ).","title":"Usage"},{"location":"guide/execution/jobconfig/context-variables/#contexts","text":"Each context corresponds to a stage in the Job's lifecycle when certain variables are made available. The following is a list of all contexts and available variables.","title":"Contexts"},{"location":"guide/execution/jobconfig/context-variables/#jobconfig","text":"Variables are evaluated when creating a Job from a JobConfig, such as during cron scheduling or ad-hoc starts of a Job. Variable Description Example jobconfig.name Name of the JobConfig that created the Job. my-example-jobconfig jobconfig.namespace Namespace of the JobConfig that created the Job. furiko jobconfig.uid UID of the JobConfig that created the Job. 4247b21e-713f-46db-b8f5-39917893577e jobconfig.cron_schedule Cron expression that was used to schedule the Job. H/15 * * * * jobconfig.cron_timezone Cron timezone that was used to schedule the Job. Asia/Singapore","title":"jobconfig"},{"location":"guide/execution/jobconfig/context-variables/#job","text":"Variables are evaluated when creating a task for a Job. This takes place after the Job is started. Variable Description Example job.name Name of the Job that created the task. my-example-jobconfig.1646586480 job.namespace Namespace of the Job that created the task. furiko job.type Type of Job, e.g. Adhoc , Scheduled . Adhoc","title":"job"},{"location":"guide/execution/jobconfig/context-variables/#task","text":"Variables are evaluated when creating a task. Variable Description Example task.name Name of the task. my-example-jobconfig.1646586480.1 task.namespace Namespace of the task. furiko task.retry_index The retry index of the task, starts from 1. 1 Additionally, the following variables are included when a parallel job is created: Variable Description Example task.index_num If withCount is used, the task index number (from 0...N-1). 2 task.index_key If withKeys is used, the task index key (as a string). item-3 task.index_matrix.<key> If withMatrix is used, the value of the corresponding key in the matrix for this task index. value-1","title":"task"},{"location":"guide/execution/jobconfig/context-variables/#option","text":"The option context is a special context that allows for user-defined inputs to parameterize the Job. For more information, see Job Options .","title":"option"},{"location":"guide/execution/jobconfig/context-variables/#substitutions","text":"Variables are evaluated based on their context, but substitution is only performed only when creating the task object (i.e. when creating the PodSpec ). Because of this, the Job exposes a substitutions field which captures all evaluated substitutions so far before finally evaluating it at the end. Users may also override certain context variables by manually specifying it in the substitutions field when creating the Job. They will take precedence over all other variables that are evaluated (context variables and option variables). For example, given the following configuration below: Example: Custom Substitutions apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : dummy-sample- spec : configName : jobconfig-sample substitutions : option.username : \"mary\" jobconfig.name : \"dummy-sample\" optionValues : |- username: \"bob\" expiry: \"168h\" option.username will be evaluated to mary ( substitutions takes precedence) option.expiry will be evaluated to 168h (from optionValues ) jobconfig.name will be evaluated to dummy-sample ( substitutions takes precedence) jobconfig.namespace will be evaluated to default (from the jobconfig context)","title":"Substitutions"},{"location":"guide/execution/jobconfig/cron-syntax/","text":"Cron Syntax Furiko supports a cron syntax that extends the standard cron rules, borrowing ideas from various cron implementations found in the wild. This page acts as the reference document for Furiko's cron syntax. Internally, Furiko uses a fork of the cronexpr library, found here: https://github.com/furiko/cronexpr Syntax Reference Fields Each cron expression consists of several tokens delimited by spaces. The following diagram explains the fields for each token. # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 second (0 - 59); optional # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6 / SAT - SUN) # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 year (1970 - 2099); optional # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 # * * * * * * * Furiko supports variable-length cron expressions, so that it accepts expressions between 5-7 tokens long. The following table explains what each tokens corresponds to, depending on the number of tokens in the expression. Length Seconds Minutes Hours Day of Month Month Day of Week Year 5 tokens 1st 2nd 3rd 4th 5th 6 tokens 1st 2nd 3rd 4th 5th 6th 7 tokens 1st 2nd 3rd 4th 5th 6th 7th Multiple Values , : Specify multiple values, e.g. MON,WED,FRI - : Specify range of values, e.g. 5-8 * : Wildcard value ? : Wildcard value For simplicity, both * and ? are treated as equivalent. In most cases, you can just use * . Intervals In EBNF notation: interval_field = [ offset ] , \"/\" , interval ; offset = start , [ \"-\" , end ] ; offset : Start counting from this value. If not specified, defaults to 0. Can also optionally specify end of range (inclusive). interval : The size of the interval between values. Examples: /15 : Every 15 units starting from 0. 0/15 : Every 15 units starting from 0, i.e. 0, 15, 30, 45 (for minutes). 3/15 : Every 15 units starting from 3, i.e. 3, 18, 33, 48 (for minutes). 0-15/3 : Every 3 units starting from 0 ending at 15 (inclusive), i.e. 0, 3, 6, 9, 12, 15. */15 : Equivalent to 0/15 . Day of Week Furiko supports both standard and Quartz cron formats. Notably, the digit values for day of week for the Quartz format differs from most other cron parsers. Digit Standard Quartz 1 MON SUN 2 TUE MON 3 WED TUE 4 THU WED 5 FRI THU 6 SAT FRI 7 SUN SAT Info Due to potential confusion, it is recommended to use MON - SAT instead of digits when specifying the day of week. Hash-based Load Balancing Furiko's cron parser implements H , which will be substituted with a number according to the hash of the JobConfig's name, so that H will produce an even load balancing of all jobs in the cluster on average. 1 The following example shows how you can use H in your cron schedule: 0 0/3 * * * : Run every 3 hours, at 00:00, 03:00, 06:00, ... H H/3 * * * : Run every 3 hours, possible schedule is 01:34, 04:34, 07:34, ... Tip It is recommended to use H tokens whenever possible. In practice, most jobs do not have a strict dependency on when they need to be run, but use a cron expression to describe an interval that it is expected to be run. There are 4 possible H formats that are introduced: Type Description Example Usage H Represents a single point in time within the time unit. H * * * * : Run once every 1 hour, at the H minute of the hour. H/2 Represents an interval that starts at the H offset. 0 H/6 * * * : Run once every 6 hours, starting between 0-5 AM and every 6 hours thereafter. H(5-19) Represents a single point in time with a range. H(5-19) * * * * : Run once every 1 hour, anytime between the 5th to 19th minute of the hour. H(5-19)/3 Represents an interval with a range offset. H(5-19)/5 * * * * : Run every 5 minutes in the 5th to 19th minute of the hour, up to a total of 3 times. Example: For H=0 : 00:05, 00:10, 00:15, 01:05, 01:10, 01:15, ... For H=2 : 00:07, 00:12, 00:17, 01:07, 01:12, 01:17, ... Note of Caution for Intervallic H Types Using intervallic H types may produce different behaviours depending on the value of H used if the denominator does not evenly divide the size of the time unit/range. Taking 0 H/7 * * * * * as an example, if H=0 it will run 9 times per hour, but if H=6 it will run only 8 times per hour. As such, it is recommended to choose intervals that are divisors of the size of the range that the H token is in. Warning for H in Day of Month The day of month field chooses a hash between the 1-28 range, so H/3 may produce a job that runs more or less frequently near the end of the month as compared to the others. This behavior comes from Jenkins, and Furiko replicates this same behavior to avoid ambiguity. The following explanation is taken from the Jenkins documentation : Beware that for the day of month field, short cycles such as */3 or H/3 will not work consistently near the end of most months, due to variable month lengths. For example, */3 will run on the 1st, 4th, \u2026 31st days of a long month, then again the next day of the next month. Hashes are always chosen in the 1-28 range, so H/3 will produce a gap between runs of between 3 and 6 days at the end of a month. (Longer cycles will also have inconsistent lengths but the effect may be relatively less noticeable.) Configuration The CronController supports some configuring additional parser options that would apply to all JobConfigs in the cluster. For detailed configuration documentation, see Execution Dynamic Config . Configuration Name Description Default cronFormat Format used to parse cron expressions \"standard\" cronHashNames Whether to enable hash-based cron expressions true cronHashSecondsByDefault Whether to treat empty seconds as H (true) or 0 (false) false cronHashFields Whether to include field type in hash true This format is first introduced in Jenkins, which has been adopted in a few other cron schedulers as well: https://www.jenkins.io/doc/book/pipeline/syntax/#cron-syntax \u21a9","title":"Cron Syntax"},{"location":"guide/execution/jobconfig/cron-syntax/#cron-syntax","text":"Furiko supports a cron syntax that extends the standard cron rules, borrowing ideas from various cron implementations found in the wild. This page acts as the reference document for Furiko's cron syntax. Internally, Furiko uses a fork of the cronexpr library, found here: https://github.com/furiko/cronexpr","title":"Cron Syntax"},{"location":"guide/execution/jobconfig/cron-syntax/#syntax-reference","text":"","title":"Syntax Reference"},{"location":"guide/execution/jobconfig/cron-syntax/#fields","text":"Each cron expression consists of several tokens delimited by spaces. The following diagram explains the fields for each token. # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 second (0 - 59); optional # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6 / SAT - SUN) # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 year (1970 - 2099); optional # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 # * * * * * * * Furiko supports variable-length cron expressions, so that it accepts expressions between 5-7 tokens long. The following table explains what each tokens corresponds to, depending on the number of tokens in the expression. Length Seconds Minutes Hours Day of Month Month Day of Week Year 5 tokens 1st 2nd 3rd 4th 5th 6 tokens 1st 2nd 3rd 4th 5th 6th 7 tokens 1st 2nd 3rd 4th 5th 6th 7th","title":"Fields"},{"location":"guide/execution/jobconfig/cron-syntax/#multiple-values","text":", : Specify multiple values, e.g. MON,WED,FRI - : Specify range of values, e.g. 5-8 * : Wildcard value ? : Wildcard value For simplicity, both * and ? are treated as equivalent. In most cases, you can just use * .","title":"Multiple Values"},{"location":"guide/execution/jobconfig/cron-syntax/#intervals","text":"In EBNF notation: interval_field = [ offset ] , \"/\" , interval ; offset = start , [ \"-\" , end ] ; offset : Start counting from this value. If not specified, defaults to 0. Can also optionally specify end of range (inclusive). interval : The size of the interval between values. Examples: /15 : Every 15 units starting from 0. 0/15 : Every 15 units starting from 0, i.e. 0, 15, 30, 45 (for minutes). 3/15 : Every 15 units starting from 3, i.e. 3, 18, 33, 48 (for minutes). 0-15/3 : Every 3 units starting from 0 ending at 15 (inclusive), i.e. 0, 3, 6, 9, 12, 15. */15 : Equivalent to 0/15 .","title":"Intervals"},{"location":"guide/execution/jobconfig/cron-syntax/#day-of-week","text":"Furiko supports both standard and Quartz cron formats. Notably, the digit values for day of week for the Quartz format differs from most other cron parsers. Digit Standard Quartz 1 MON SUN 2 TUE MON 3 WED TUE 4 THU WED 5 FRI THU 6 SAT FRI 7 SUN SAT Info Due to potential confusion, it is recommended to use MON - SAT instead of digits when specifying the day of week.","title":"Day of Week"},{"location":"guide/execution/jobconfig/cron-syntax/#hash-based-load-balancing","text":"Furiko's cron parser implements H , which will be substituted with a number according to the hash of the JobConfig's name, so that H will produce an even load balancing of all jobs in the cluster on average. 1 The following example shows how you can use H in your cron schedule: 0 0/3 * * * : Run every 3 hours, at 00:00, 03:00, 06:00, ... H H/3 * * * : Run every 3 hours, possible schedule is 01:34, 04:34, 07:34, ... Tip It is recommended to use H tokens whenever possible. In practice, most jobs do not have a strict dependency on when they need to be run, but use a cron expression to describe an interval that it is expected to be run. There are 4 possible H formats that are introduced: Type Description Example Usage H Represents a single point in time within the time unit. H * * * * : Run once every 1 hour, at the H minute of the hour. H/2 Represents an interval that starts at the H offset. 0 H/6 * * * : Run once every 6 hours, starting between 0-5 AM and every 6 hours thereafter. H(5-19) Represents a single point in time with a range. H(5-19) * * * * : Run once every 1 hour, anytime between the 5th to 19th minute of the hour. H(5-19)/3 Represents an interval with a range offset. H(5-19)/5 * * * * : Run every 5 minutes in the 5th to 19th minute of the hour, up to a total of 3 times. Example: For H=0 : 00:05, 00:10, 00:15, 01:05, 01:10, 01:15, ... For H=2 : 00:07, 00:12, 00:17, 01:07, 01:12, 01:17, ... Note of Caution for Intervallic H Types Using intervallic H types may produce different behaviours depending on the value of H used if the denominator does not evenly divide the size of the time unit/range. Taking 0 H/7 * * * * * as an example, if H=0 it will run 9 times per hour, but if H=6 it will run only 8 times per hour. As such, it is recommended to choose intervals that are divisors of the size of the range that the H token is in. Warning for H in Day of Month The day of month field chooses a hash between the 1-28 range, so H/3 may produce a job that runs more or less frequently near the end of the month as compared to the others. This behavior comes from Jenkins, and Furiko replicates this same behavior to avoid ambiguity. The following explanation is taken from the Jenkins documentation : Beware that for the day of month field, short cycles such as */3 or H/3 will not work consistently near the end of most months, due to variable month lengths. For example, */3 will run on the 1st, 4th, \u2026 31st days of a long month, then again the next day of the next month. Hashes are always chosen in the 1-28 range, so H/3 will produce a gap between runs of between 3 and 6 days at the end of a month. (Longer cycles will also have inconsistent lengths but the effect may be relatively less noticeable.)","title":"Hash-based Load Balancing"},{"location":"guide/execution/jobconfig/cron-syntax/#configuration","text":"The CronController supports some configuring additional parser options that would apply to all JobConfigs in the cluster. For detailed configuration documentation, see Execution Dynamic Config . Configuration Name Description Default cronFormat Format used to parse cron expressions \"standard\" cronHashNames Whether to enable hash-based cron expressions true cronHashSecondsByDefault Whether to treat empty seconds as H (true) or 0 (false) false cronHashFields Whether to include field type in hash true This format is first introduced in Jenkins, which has been adopted in a few other cron schedulers as well: https://www.jenkins.io/doc/book/pipeline/syntax/#cron-syntax \u21a9","title":"Configuration"},{"location":"guide/execution/jobconfig/job-options/","text":"Job Options This page introduces the concept of Job Options, which allows Furiko users to parameterize their jobs with structured user-defined input. Note that a Job Option is a special type of Context Variable ; refer to the linked page to see how they work. Overview Introduction A job option looks like ${option.my_option_name} , where the option name my_option_name is a custom variable defined in the JobConfig. Users who are familiar with Rundeck may find this concept familiar, as it was inspired by the corresponding feature in Rundeck: https://docs.rundeck.com/docs/manual/job-options.html Usage Job options have to be first defined in the JobConfig, like so: Example JobConfigSpec apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig metadata : generateName : jobconfig-sample spec : option : options : - type : String name : username label : Username string : default : Guest User trimSpaces : true This declares an option option.username that can be used in the task template, with a default value of Guest User . You can then utilize the option value like so: Example PodTemplateSpec (in TaskTemplate) spec : containers : - name : job-container args : - echo - \"Hello world, ${option.username}!\" To pass option value inputs into the JobConfig, you can do so with optionValues : Example JobSpec apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- username: John Default Values Some option types specify a default field in the config which defines a default value when not specified. This will be used when a Job is started automatically, e.g. on a cron schedule. Option Configuration type The type of option must be one of the allowed values below. Name Input Type Description String string Arbitary string input Bool bool Boolean input Select string String input from a list of choices Multi []string List of strings, input from a list of choices Date string RFC3339-formatted date/time string Additional option configuration may apply for specific option types. Each option type above links to addition configuration documentation, as well as example configuration, input specification and output values. name The canonical name of the option, like my_option_name , which will be used during substitution of the task template. Should not contain spaces or special characters other than underscore, hyphen and dot. label An optional human-readable label of the option, like My Option . This will be displayed only in human-readable interfaces. If not specified, it will default to name . required Defines whether this option is required. Does not apply to bool options. When used in conjunction with default (available in some option types), if there is no default value AND there is not input value specified via optionValues , the Job will fail validation and will not be created. Info Jobs which are started on a scheduled basis will fail if required is true but there is no default value. Option Types String Options The String option type allows you to specify an option that accepts arbitrary string input. Config Type Description default string Default value if not specified. If empty and required , will throw a validation error. trimSpaces bool If true, the spaces will be trimmed from the input value (start and end) before substitution. Example Usage Example Option Configuration - type : String name : username label : Username string : default : Example User trimSpaces : true Example optionValues spec : optionValues : |- username: johnw203 Example Output johnw203 Boolean Options The Bool option type allows you to specify an option that has only two possible values. Config Type Description default bool Default value ( true or false ) if not specified. format BoolOptionFormat Format type to render the bool option value as a string. Use custom to specify custom true/false format. trueVal string Custom string to format if value is true . Only applicable to custom format. falseVal string Custom string to format if value is false . Only applicable to custom format. BoolOptionFormat The following is a list of pre-defined BoolOptionFormat : Format True False TrueFalse (default) true false YesNo yes no OneZero 1 0 Custom (custom) (custom) If none of the above suit your needs, you can use Custom to specify a custom trueVal and falseVal . Tip Using a bool option type is most useful for specifying command-line flags (e.g. --verbose ), see the example below. Example Usage Example Option Configuration - type : Bool name : verbose label : \"Verbose?\" bool : format : Custom trueVal : \"--verbose\" falseVal : \"\" Example optionValues spec : optionValues : |- verbose: true Example Output --verbose Select Options The select option type allows you to specify an option whose values belong to a fixed set of values. Config Type Description default string Default value if not specified. If empty and required , will throw a validation error. Must be one of the values in values . values []string List of allowed values to choose from. allowCustom bool If false, then input values not found in values will be rejected. Example Usage Example Option Configuration - type : Select name : person label : Person Name select : default : Bob values : - Mary - Bob - Alice allowCustom : true Example optionValues spec : optionValues : |- person: Mary Example Output Mary Multi-select Options The multi option type allows you to specify an option whose values belong to a fixed set of values, where users can specify zero or more values. Config Type Description default []string Default set of values if not specified. If empty and required , will throw a validation error. Must be one of the values in values . values []string List of allowed values to choose from. delimiter string String to delimit values by. allowCustom bool If false, then input values not found in values will be rejected. Example Usage Example Option Configuration - type : Multi name : targets label : Targets multi : default : - mysql - redis values : - mysql - redis - s3 - kafka delimiter : \",\" allowCustom : true Example optionValues spec : optionValues : |- targets: - kafka - s3 Example Output kafka,s3 Date Options The date option type allows you to specify an option which accepts a date/time with timezone. If not set, the default value is an empty string. Timezones The timezone of the formatted date string follows that of the input option. A full example with timezone is described below. Example Option Configuration - type : Date name : startTime label : Start Time date : format : HH:mm:ss Example optionValues spec : # We specify 02:30 AM in Indian Standard Time. optionValues : |- startTime: \"2022-03-14T02:30:00+05:30\" Example Output # It is formatted in the same timezone as the input, thus producing 02:30 AM. 02:30:00 As such, if the output value is required to be timezone-aware, we strongly recommend using the full RFC3339-formatted date string and parsing it in your application. If an exact point in time is desired (i.e. not timezone-aware), we recommend using X for the date format (which produces the Unix timestamp). Config Type Description format string Moment.js format string to format the input date. If not specified, defaults to RFC3339 format ( YYYY-MM-DDTHH:mm:ssZ ). Example Usage Example Option Configuration - type : Date name : fromDate label : From Date date : format : YYYY-MM-DD Example optionValues spec : optionValues : |- fromDate: \"2022-01-01T00:02:30Z\" Example Output 2022-01-01","title":"Job Options"},{"location":"guide/execution/jobconfig/job-options/#job-options","text":"This page introduces the concept of Job Options, which allows Furiko users to parameterize their jobs with structured user-defined input. Note that a Job Option is a special type of Context Variable ; refer to the linked page to see how they work.","title":"Job Options"},{"location":"guide/execution/jobconfig/job-options/#overview","text":"","title":"Overview"},{"location":"guide/execution/jobconfig/job-options/#introduction","text":"A job option looks like ${option.my_option_name} , where the option name my_option_name is a custom variable defined in the JobConfig. Users who are familiar with Rundeck may find this concept familiar, as it was inspired by the corresponding feature in Rundeck: https://docs.rundeck.com/docs/manual/job-options.html","title":"Introduction"},{"location":"guide/execution/jobconfig/job-options/#usage","text":"Job options have to be first defined in the JobConfig, like so: Example JobConfigSpec apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig metadata : generateName : jobconfig-sample spec : option : options : - type : String name : username label : Username string : default : Guest User trimSpaces : true This declares an option option.username that can be used in the task template, with a default value of Guest User . You can then utilize the option value like so: Example PodTemplateSpec (in TaskTemplate) spec : containers : - name : job-container args : - echo - \"Hello world, ${option.username}!\" To pass option value inputs into the JobConfig, you can do so with optionValues : Example JobSpec apiVersion : execution.furiko.io/v1alpha1 kind : Job metadata : generateName : jobconfig-sample- spec : configName : jobconfig-sample optionValues : |- username: John","title":"Usage"},{"location":"guide/execution/jobconfig/job-options/#default-values","text":"Some option types specify a default field in the config which defines a default value when not specified. This will be used when a Job is started automatically, e.g. on a cron schedule.","title":"Default Values"},{"location":"guide/execution/jobconfig/job-options/#option-configuration","text":"","title":"Option Configuration"},{"location":"guide/execution/jobconfig/job-options/#type","text":"The type of option must be one of the allowed values below. Name Input Type Description String string Arbitary string input Bool bool Boolean input Select string String input from a list of choices Multi []string List of strings, input from a list of choices Date string RFC3339-formatted date/time string Additional option configuration may apply for specific option types. Each option type above links to addition configuration documentation, as well as example configuration, input specification and output values.","title":"type"},{"location":"guide/execution/jobconfig/job-options/#name","text":"The canonical name of the option, like my_option_name , which will be used during substitution of the task template. Should not contain spaces or special characters other than underscore, hyphen and dot.","title":"name"},{"location":"guide/execution/jobconfig/job-options/#label","text":"An optional human-readable label of the option, like My Option . This will be displayed only in human-readable interfaces. If not specified, it will default to name .","title":"label"},{"location":"guide/execution/jobconfig/job-options/#required","text":"Defines whether this option is required. Does not apply to bool options. When used in conjunction with default (available in some option types), if there is no default value AND there is not input value specified via optionValues , the Job will fail validation and will not be created. Info Jobs which are started on a scheduled basis will fail if required is true but there is no default value.","title":"required"},{"location":"guide/execution/jobconfig/job-options/#option-types","text":"","title":"Option Types"},{"location":"guide/execution/jobconfig/job-options/#string-options","text":"The String option type allows you to specify an option that accepts arbitrary string input. Config Type Description default string Default value if not specified. If empty and required , will throw a validation error. trimSpaces bool If true, the spaces will be trimmed from the input value (start and end) before substitution. Example Usage Example Option Configuration - type : String name : username label : Username string : default : Example User trimSpaces : true Example optionValues spec : optionValues : |- username: johnw203 Example Output johnw203","title":"String Options"},{"location":"guide/execution/jobconfig/job-options/#boolean-options","text":"The Bool option type allows you to specify an option that has only two possible values. Config Type Description default bool Default value ( true or false ) if not specified. format BoolOptionFormat Format type to render the bool option value as a string. Use custom to specify custom true/false format. trueVal string Custom string to format if value is true . Only applicable to custom format. falseVal string Custom string to format if value is false . Only applicable to custom format.","title":"Boolean Options"},{"location":"guide/execution/jobconfig/job-options/#booloptionformat","text":"The following is a list of pre-defined BoolOptionFormat : Format True False TrueFalse (default) true false YesNo yes no OneZero 1 0 Custom (custom) (custom) If none of the above suit your needs, you can use Custom to specify a custom trueVal and falseVal . Tip Using a bool option type is most useful for specifying command-line flags (e.g. --verbose ), see the example below. Example Usage Example Option Configuration - type : Bool name : verbose label : \"Verbose?\" bool : format : Custom trueVal : \"--verbose\" falseVal : \"\" Example optionValues spec : optionValues : |- verbose: true Example Output --verbose","title":"BoolOptionFormat"},{"location":"guide/execution/jobconfig/job-options/#select-options","text":"The select option type allows you to specify an option whose values belong to a fixed set of values. Config Type Description default string Default value if not specified. If empty and required , will throw a validation error. Must be one of the values in values . values []string List of allowed values to choose from. allowCustom bool If false, then input values not found in values will be rejected. Example Usage Example Option Configuration - type : Select name : person label : Person Name select : default : Bob values : - Mary - Bob - Alice allowCustom : true Example optionValues spec : optionValues : |- person: Mary Example Output Mary","title":"Select Options"},{"location":"guide/execution/jobconfig/job-options/#multi-select-options","text":"The multi option type allows you to specify an option whose values belong to a fixed set of values, where users can specify zero or more values. Config Type Description default []string Default set of values if not specified. If empty and required , will throw a validation error. Must be one of the values in values . values []string List of allowed values to choose from. delimiter string String to delimit values by. allowCustom bool If false, then input values not found in values will be rejected. Example Usage Example Option Configuration - type : Multi name : targets label : Targets multi : default : - mysql - redis values : - mysql - redis - s3 - kafka delimiter : \",\" allowCustom : true Example optionValues spec : optionValues : |- targets: - kafka - s3 Example Output kafka,s3","title":"Multi-select Options"},{"location":"guide/execution/jobconfig/job-options/#date-options","text":"The date option type allows you to specify an option which accepts a date/time with timezone. If not set, the default value is an empty string. Timezones The timezone of the formatted date string follows that of the input option. A full example with timezone is described below. Example Option Configuration - type : Date name : startTime label : Start Time date : format : HH:mm:ss Example optionValues spec : # We specify 02:30 AM in Indian Standard Time. optionValues : |- startTime: \"2022-03-14T02:30:00+05:30\" Example Output # It is formatted in the same timezone as the input, thus producing 02:30 AM. 02:30:00 As such, if the output value is required to be timezone-aware, we strongly recommend using the full RFC3339-formatted date string and parsing it in your application. If an exact point in time is desired (i.e. not timezone-aware), we recommend using X for the date format (which produces the Unix timestamp). Config Type Description format string Moment.js format string to format the input date. If not specified, defaults to RFC3339 format ( YYYY-MM-DDTHH:mm:ssZ ). Example Usage Example Option Configuration - type : Date name : fromDate label : From Date date : format : YYYY-MM-DD Example optionValues spec : optionValues : |- fromDate: \"2022-01-01T00:02:30Z\" Example Output 2022-01-01","title":"Date Options"},{"location":"guide/execution/jobconfig/sample-configuration/","text":"Sample Configuration The following is a full sample configuration. apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig metadata : name : jobconfig-sample spec : # Concurrency configuration. concurrency : policy : Forbid # Schedule configuration. schedule : cron : expression : \"0 */15 * * * * *\" timezone : \"Asia/Singapore\" disabled : false # Job options. option : options : - type : String name : username label : Username string : default : Example User trimSpaces : true # Template for the Job to be created. For more info, see the Job sample configuration. template : # Any labels and annotations will be automatically added to downstream Jobs. metadata : annotations : annotations.furiko.io/job-group : \"cool-jobs\" spec : # Specifies maximum number of attempts for each task, defaults to 1. maxAttempts : 3 # Optional delay between each task retry. retryDelaySeconds : 10 # Optional duration in seconds for how long each task should be pending for # until it gets killed. taskPendingTimeoutSeconds : 1800 # The template for each task to be created by the Job. taskTemplate : # Specify how to create the task as a Pod. This is just a PodTemplateSpec. pod : spec : containers : # Notice how we can use context variables and job options inside # the PodSpec freely to be substituted at runtime. - name : job-container args : - echo - \"Hello world, ${option.username}!\" env : - name : JOBCONFIG_NAME value : \"${jobconfig.name}\" - name : JOB_NAME value : \"${job.name}\" image : \"alpine\" resources : limits : cpu : 100m memory : 64Mi","title":"Sample Configuration"},{"location":"guide/execution/jobconfig/sample-configuration/#sample-configuration","text":"The following is a full sample configuration. apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig metadata : name : jobconfig-sample spec : # Concurrency configuration. concurrency : policy : Forbid # Schedule configuration. schedule : cron : expression : \"0 */15 * * * * *\" timezone : \"Asia/Singapore\" disabled : false # Job options. option : options : - type : String name : username label : Username string : default : Example User trimSpaces : true # Template for the Job to be created. For more info, see the Job sample configuration. template : # Any labels and annotations will be automatically added to downstream Jobs. metadata : annotations : annotations.furiko.io/job-group : \"cool-jobs\" spec : # Specifies maximum number of attempts for each task, defaults to 1. maxAttempts : 3 # Optional delay between each task retry. retryDelaySeconds : 10 # Optional duration in seconds for how long each task should be pending for # until it gets killed. taskPendingTimeoutSeconds : 1800 # The template for each task to be created by the Job. taskTemplate : # Specify how to create the task as a Pod. This is just a PodTemplateSpec. pod : spec : containers : # Notice how we can use context variables and job options inside # the PodSpec freely to be substituted at runtime. - name : job-container args : - echo - \"Hello world, ${option.username}!\" env : - name : JOBCONFIG_NAME value : \"${jobconfig.name}\" - name : JOB_NAME value : \"${job.name}\" image : \"alpine\" resources : limits : cpu : 100m memory : 64Mi","title":"Sample Configuration"},{"location":"guide/execution/jobconfig/scheduling/","text":"Scheduling A JobConfig can be set to automatically create Jobs based on a cron schedule . Sample Configuration apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig spec : schedule : cron : expression : 0 10 * * * timezone : America/New_York disabled : false Configuration Options cron cron.expression Defines a cron expression that defines when the JobConfig should be run. Furiko uses an extended cron syntax that supports between 5 to 7 tokens, supporting up to second-level granularity of cron expressions. Additionally, Furiko's cron syntax also supports H tokens for load balancing. For more details and examples, see Cron Syntax . cron.timezone Defines an optional value that defines the timezone that cron.expression should be interpreted in. For example, a cron schedule of 0 10 * * * with a timezone of Asia/Singapore will be interpreted as running at 02:00:00 UTC every day. timezone must be one of the following values: A valid tz string (e.g. Asia/Singapore , America/New_York ) in the Time Zone Database . A UTC offset with minutes (e.g. UTC-10:00 ). A GMT offset with minutes (e.g. GMT+05:30 ). The meaning is the same as its UTC counterpart. If not specified, defaults to the controller's global default configuration value (via defaultTimezone ), which defaults to UTC . Info This field merely is used for parsing cron.expression , and has nothing to do with /etc/timezone inside the container (i.e. it will not set $TZ automatically). disabled Automatic scheduling can also be disabled or suspended, by specifying disabled: true . constraints Specify any constraints that should apply to the schedule. schedule.notBefore If specified, automatic scheduling will not take place before this timestamp. Useful to specify a JobConfig that should only start scheduling only after a certain date in the future. schedule.notAfter If specified, automatic scheduling will not take place after this timestamp. Useful to specify a JobConfig that should stop scheduling after a certain date in the future. Handling Concurrent Jobs Jobs may not be scheduled immediately (or forbidden entirely) based on the concurrency policy if there are concurrent executions of Jobs belonging to this JobConfig. For more details, see Concurrency . Once-off Scheduling A JobConfig can also be scheduled to run once-off in the future by creating a Job with a startPolicy . See Adhoc Execution and Start Policy for more details. Back-scheduling The CronController supports \"back-scheduling\" Jobs to be created even after its schedule has passed but the controller had detected that it had failed to create one at that point in time. Given a JobConfig with a cron.expression of */5 * * * * , the following diagram illustrates an example of what it looks like: This effectively allows the controller to survive short periods of downtime with little repercussions. In practice, most jobs are able to tolerate a few minutes of delay, and it would be more costly to skip the job when it should have been scheduled. Tip This feature allows the administrator to safely upgrade Furiko at any time, without having to find a time to restart the process that would cause minimal disruption. Global Configuration The back-scheduling thresholds can be further tuned according to you or your organization's requirements. maxDowntimeThresholdSeconds : Defines the maximum downtime beyond which back-scheduling will not take place. maxMissedSchedules : Defines the maximum number of missed schedules per JobConfig that the controller will attempt to back-schedule. For more information, refer to the Execution Dynamic Configuration reference.","title":"Scheduling"},{"location":"guide/execution/jobconfig/scheduling/#scheduling","text":"A JobConfig can be set to automatically create Jobs based on a cron schedule .","title":"Scheduling"},{"location":"guide/execution/jobconfig/scheduling/#sample-configuration","text":"apiVersion : execution.furiko.io/v1alpha1 kind : JobConfig spec : schedule : cron : expression : 0 10 * * * timezone : America/New_York disabled : false","title":"Sample Configuration"},{"location":"guide/execution/jobconfig/scheduling/#configuration-options","text":"","title":"Configuration Options"},{"location":"guide/execution/jobconfig/scheduling/#cron","text":"","title":"cron"},{"location":"guide/execution/jobconfig/scheduling/#cronexpression","text":"Defines a cron expression that defines when the JobConfig should be run. Furiko uses an extended cron syntax that supports between 5 to 7 tokens, supporting up to second-level granularity of cron expressions. Additionally, Furiko's cron syntax also supports H tokens for load balancing. For more details and examples, see Cron Syntax .","title":"cron.expression"},{"location":"guide/execution/jobconfig/scheduling/#crontimezone","text":"Defines an optional value that defines the timezone that cron.expression should be interpreted in. For example, a cron schedule of 0 10 * * * with a timezone of Asia/Singapore will be interpreted as running at 02:00:00 UTC every day. timezone must be one of the following values: A valid tz string (e.g. Asia/Singapore , America/New_York ) in the Time Zone Database . A UTC offset with minutes (e.g. UTC-10:00 ). A GMT offset with minutes (e.g. GMT+05:30 ). The meaning is the same as its UTC counterpart. If not specified, defaults to the controller's global default configuration value (via defaultTimezone ), which defaults to UTC . Info This field merely is used for parsing cron.expression , and has nothing to do with /etc/timezone inside the container (i.e. it will not set $TZ automatically).","title":"cron.timezone"},{"location":"guide/execution/jobconfig/scheduling/#disabled","text":"Automatic scheduling can also be disabled or suspended, by specifying disabled: true .","title":"disabled"},{"location":"guide/execution/jobconfig/scheduling/#constraints","text":"Specify any constraints that should apply to the schedule.","title":"constraints"},{"location":"guide/execution/jobconfig/scheduling/#schedulenotbefore","text":"If specified, automatic scheduling will not take place before this timestamp. Useful to specify a JobConfig that should only start scheduling only after a certain date in the future.","title":"schedule.notBefore"},{"location":"guide/execution/jobconfig/scheduling/#schedulenotafter","text":"If specified, automatic scheduling will not take place after this timestamp. Useful to specify a JobConfig that should stop scheduling after a certain date in the future.","title":"schedule.notAfter"},{"location":"guide/execution/jobconfig/scheduling/#handling-concurrent-jobs","text":"Jobs may not be scheduled immediately (or forbidden entirely) based on the concurrency policy if there are concurrent executions of Jobs belonging to this JobConfig. For more details, see Concurrency .","title":"Handling Concurrent Jobs"},{"location":"guide/execution/jobconfig/scheduling/#once-off-scheduling","text":"A JobConfig can also be scheduled to run once-off in the future by creating a Job with a startPolicy . See Adhoc Execution and Start Policy for more details.","title":"Once-off Scheduling"},{"location":"guide/execution/jobconfig/scheduling/#back-scheduling","text":"The CronController supports \"back-scheduling\" Jobs to be created even after its schedule has passed but the controller had detected that it had failed to create one at that point in time. Given a JobConfig with a cron.expression of */5 * * * * , the following diagram illustrates an example of what it looks like: This effectively allows the controller to survive short periods of downtime with little repercussions. In practice, most jobs are able to tolerate a few minutes of delay, and it would be more costly to skip the job when it should have been scheduled. Tip This feature allows the administrator to safely upgrade Furiko at any time, without having to find a time to restart the process that would cause minimal disruption.","title":"Back-scheduling"},{"location":"guide/execution/jobconfig/scheduling/#global-configuration","text":"The back-scheduling thresholds can be further tuned according to you or your organization's requirements. maxDowntimeThresholdSeconds : Defines the maximum downtime beyond which back-scheduling will not take place. maxMissedSchedules : Defines the maximum number of missed schedules per JobConfig that the controller will attempt to back-schedule. For more information, refer to the Execution Dynamic Configuration reference.","title":"Global Configuration"},{"location":"guide/setup/advanced-installation/","text":"Advanced Installation This guide explains how to perform a full manual installation of Furiko, and also serves as a full explainer for the recommended YAML method of installation . The following walkthrough assumes a sufficient understanding of various Kubernetes concepts and knowledge to administer a Kubernetes cluster. In addition, it also assumes that you have understood the architecture of Furiko and its various components. Manually Installing Furiko To start, first download furiko-execution.yaml from the relevant version on the GitHub Releases page. There are two services that will need to be installed as part of Furiko Execution: execution-controller : Responsible for managing and interacting with Execution CRDs execution-webhooks : Responsible for enforcing data mutation and validation of Execution CRDs Create Namespace Create a namespace for Furiko's control plane components. In the YAML, this is furiko-system , which is meant to be shared across all Furiko components. Install CRDs Copy and install the CustomResourceDefinition resources specified in the YAML. The following CRDs will be needed by Furiko Execution: jobconfigs.execution.furiko.io jobs.execution.furiko.io Setup RBAC We need to set up RBAC for the execution-controller and execution-webhook services. Create the correct ClusterRole and Role resources. The RBAC manifests in the YAML can serve as a guide for the minimum permissions needed. Create ServiceAccount (s) for the two components, and bind the RBAC with ClusterRoleBinding and RoleBinding respectively. Refer to the YAML for the full list of minimum required permissions. Build Custom Container Images (optional) This step can be skipped if you are alright with using the images released on Docker Hub. If you wish to customize the container image, you can download the binaries directly from the GitHub Releases page to be bundled in your own Docker image. Setup Deployments We can set up the Deployment for execution-controller and execution-webhook now. The Deployment manifests in the YAML can serve as a guide for reference. Bootstrap Configuration Both execution-controller and execution-webhook requires Bootstrap Configuration loaded from a YAML or JSON file at startup, using the --config command-line flag. For example, this can be loaded via a configMap volume mount. Refer to the following configuration pages for each of the above binaries: execution-controller Configuration execution-webhook Configuration Container Configuration Use the custom container images from the previous step if you built your own images. You can customize other configuration for the Deployment , including the command , args and securityContext . By default, images on Docker Hub are rootless and thus runAsNonRoot is enabled. If using a custom Docker image you may need to disable it. The liveness and readiness probes default to the HTTP server at port 8080 for both execution-controller and execution-webhook . To customize the port and paths, refer to the Bootstrap Configuration . You can also increase or update the resource limits for the containers if needed. Setup Services Once created, you can create the corresponding Service resources as well. Only execution-webhook requires a Service , since it will be used as an admission webhook server that extends the Kubernetes API server. Info The YAML method uses a ClusterIP service type, which requires a Container Network Interface (CNI) network add-on to be installed in your cluster. It may be possible to run execution-webhook using LoadBalancer , which could potentially route traffic from kube-apiserver to execution-controller over WAN, incurring high latency and greater network instability in general. Another method may be to use hostNetwork and run execution-controller on the same machines as kube-apiserver itself. Both of the alternative methods mentioned are not officially supported by Furiko. By default, execution-webhook listens on port 8443 (can be changed via bootstrap configuration). The server listens on HTTPS, which is required in order to be used by the Kubernetes API server for webhooks. Refer to the following section on further HTTPS setup. Provisioning Webhook Certificates Traffic between the Kubernetes API server and admission webhook servers must be using HTTPS, so we will need to provision TLS certificates that will be served by execution-webhook . In the YAML installation method, a Job is used to automatically provision a self-signed certificate (valid for 100 years), store it in a Secret, and patch the webhook configurations with the certificate's public key. This functionality is provided by jet/kube-webhook-certgen , and is the recommended approach taken by Furiko since no external add-ons are required. Alternative methods include using a static TLS certificate, or utilizing cluster add-ons such as cert-manager . Setup Webhook Configuration We can now create webhook configuration resources for execution-webhook . Both ValidatingWebhookConfiguration and MutatingWebhookConfiguration resources will need to be installed. Refer to the YAML for an example. The webhook server expects to receive webhook requests at well-defined paths, that include the canonical name of the CustomResourceDefinition. For example, to validate a Job , the following path is used: /validating/jobs.execution.furiko.io On the other hand, to mutate a JobConfig , the following path is used: /mutating/jobconfigs.execution.furiko.io Refer to the official Kubernetes documentation on admission webhooks to manually configure the caBundle of the webhook configuration if needed. Additional Setup Prometheus Metrics All components, including execution-controller and execution-webhook , expose Prometheus metrics that provide insight into the status of the service. In the YAML installation method, kube-rbac-proxy is used as a sidecar to restrict access to the /metrics endpoint for enhanced security. An additional Service is also created to expose the metrics using ClusterIP . More information on the setup can be found in the bootstrap configuration . Customize Dynamic Configuration Furiko also offers a dynamic configuration mechanism to configure components without requiring a restart. This is typically loaded from a ConfigMap , such that any changes to the configuration can take effect immediately. More information can be found on the dynamic configuration section of the documentation.","title":"Advanced Installation"},{"location":"guide/setup/advanced-installation/#advanced-installation","text":"This guide explains how to perform a full manual installation of Furiko, and also serves as a full explainer for the recommended YAML method of installation . The following walkthrough assumes a sufficient understanding of various Kubernetes concepts and knowledge to administer a Kubernetes cluster. In addition, it also assumes that you have understood the architecture of Furiko and its various components.","title":"Advanced Installation"},{"location":"guide/setup/advanced-installation/#manually-installing-furiko","text":"To start, first download furiko-execution.yaml from the relevant version on the GitHub Releases page. There are two services that will need to be installed as part of Furiko Execution: execution-controller : Responsible for managing and interacting with Execution CRDs execution-webhooks : Responsible for enforcing data mutation and validation of Execution CRDs","title":"Manually Installing Furiko"},{"location":"guide/setup/advanced-installation/#create-namespace","text":"Create a namespace for Furiko's control plane components. In the YAML, this is furiko-system , which is meant to be shared across all Furiko components.","title":"Create Namespace"},{"location":"guide/setup/advanced-installation/#install-crds","text":"Copy and install the CustomResourceDefinition resources specified in the YAML. The following CRDs will be needed by Furiko Execution: jobconfigs.execution.furiko.io jobs.execution.furiko.io","title":"Install CRDs"},{"location":"guide/setup/advanced-installation/#setup-rbac","text":"We need to set up RBAC for the execution-controller and execution-webhook services. Create the correct ClusterRole and Role resources. The RBAC manifests in the YAML can serve as a guide for the minimum permissions needed. Create ServiceAccount (s) for the two components, and bind the RBAC with ClusterRoleBinding and RoleBinding respectively. Refer to the YAML for the full list of minimum required permissions.","title":"Setup RBAC"},{"location":"guide/setup/advanced-installation/#build-custom-container-images-optional","text":"This step can be skipped if you are alright with using the images released on Docker Hub. If you wish to customize the container image, you can download the binaries directly from the GitHub Releases page to be bundled in your own Docker image.","title":"Build Custom Container Images (optional)"},{"location":"guide/setup/advanced-installation/#setup-deployments","text":"We can set up the Deployment for execution-controller and execution-webhook now. The Deployment manifests in the YAML can serve as a guide for reference.","title":"Setup Deployments"},{"location":"guide/setup/advanced-installation/#bootstrap-configuration","text":"Both execution-controller and execution-webhook requires Bootstrap Configuration loaded from a YAML or JSON file at startup, using the --config command-line flag. For example, this can be loaded via a configMap volume mount. Refer to the following configuration pages for each of the above binaries: execution-controller Configuration execution-webhook Configuration","title":"Bootstrap Configuration"},{"location":"guide/setup/advanced-installation/#container-configuration","text":"Use the custom container images from the previous step if you built your own images. You can customize other configuration for the Deployment , including the command , args and securityContext . By default, images on Docker Hub are rootless and thus runAsNonRoot is enabled. If using a custom Docker image you may need to disable it. The liveness and readiness probes default to the HTTP server at port 8080 for both execution-controller and execution-webhook . To customize the port and paths, refer to the Bootstrap Configuration . You can also increase or update the resource limits for the containers if needed.","title":"Container Configuration"},{"location":"guide/setup/advanced-installation/#setup-services","text":"Once created, you can create the corresponding Service resources as well. Only execution-webhook requires a Service , since it will be used as an admission webhook server that extends the Kubernetes API server. Info The YAML method uses a ClusterIP service type, which requires a Container Network Interface (CNI) network add-on to be installed in your cluster. It may be possible to run execution-webhook using LoadBalancer , which could potentially route traffic from kube-apiserver to execution-controller over WAN, incurring high latency and greater network instability in general. Another method may be to use hostNetwork and run execution-controller on the same machines as kube-apiserver itself. Both of the alternative methods mentioned are not officially supported by Furiko. By default, execution-webhook listens on port 8443 (can be changed via bootstrap configuration). The server listens on HTTPS, which is required in order to be used by the Kubernetes API server for webhooks. Refer to the following section on further HTTPS setup.","title":"Setup Services"},{"location":"guide/setup/advanced-installation/#provisioning-webhook-certificates","text":"Traffic between the Kubernetes API server and admission webhook servers must be using HTTPS, so we will need to provision TLS certificates that will be served by execution-webhook . In the YAML installation method, a Job is used to automatically provision a self-signed certificate (valid for 100 years), store it in a Secret, and patch the webhook configurations with the certificate's public key. This functionality is provided by jet/kube-webhook-certgen , and is the recommended approach taken by Furiko since no external add-ons are required. Alternative methods include using a static TLS certificate, or utilizing cluster add-ons such as cert-manager .","title":"Provisioning Webhook Certificates"},{"location":"guide/setup/advanced-installation/#setup-webhook-configuration","text":"We can now create webhook configuration resources for execution-webhook . Both ValidatingWebhookConfiguration and MutatingWebhookConfiguration resources will need to be installed. Refer to the YAML for an example. The webhook server expects to receive webhook requests at well-defined paths, that include the canonical name of the CustomResourceDefinition. For example, to validate a Job , the following path is used: /validating/jobs.execution.furiko.io On the other hand, to mutate a JobConfig , the following path is used: /mutating/jobconfigs.execution.furiko.io Refer to the official Kubernetes documentation on admission webhooks to manually configure the caBundle of the webhook configuration if needed.","title":"Setup Webhook Configuration"},{"location":"guide/setup/advanced-installation/#additional-setup","text":"","title":"Additional Setup"},{"location":"guide/setup/advanced-installation/#prometheus-metrics","text":"All components, including execution-controller and execution-webhook , expose Prometheus metrics that provide insight into the status of the service. In the YAML installation method, kube-rbac-proxy is used as a sidecar to restrict access to the /metrics endpoint for enhanced security. An additional Service is also created to expose the metrics using ClusterIP . More information on the setup can be found in the bootstrap configuration .","title":"Prometheus Metrics"},{"location":"guide/setup/advanced-installation/#customize-dynamic-configuration","text":"Furiko also offers a dynamic configuration mechanism to configure components without requiring a restart. This is typically loaded from a ConfigMap , such that any changes to the configuration can take effect immediately. More information can be found on the dynamic configuration section of the documentation.","title":"Customize Dynamic Configuration"},{"location":"guide/setup/cli/","text":"Using the CLI This guide provides details on how to install and use the Furiko CLI tool, furiko . Installation Instructions Currently, furiko is only released via Furiko's GitHub releases. You can install furiko by downloading the executable binary from Furiko's GitHub releases page directly. Download the binary that matches your system's OS and architecture (e.g. furiko_darwin_amd64 ). Rename the binary to furiko and make it executable: chmod +x furiko Usage Instructions Running furiko --help will list all available commands for the CLI tool. $ furiko --help Command-line utility to manage Furiko. Usage: furiko [ command ] Available Commands: completion generate the autocompletion script for the specified shell disable Disable automatic scheduling for a JobConfig. enable Enable automatic scheduling for a JobConfig. get Get one or more resources by name. help Help about any command kill Kill an ongoing Job. list List all resources by kind. run Run a new Job. Flags: --dynamic-config-name string Overrides the name of the dynamic cluster config. ( default \"execution-dynamic-config\" ) --dynamic-config-namespace string Overrides the namespace of the dynamic cluster config. ( default \"furiko-system\" ) -h, --help help for furiko --kubeconfig string Path to the kubeconfig file to use for CLI requests. -n, --namespace string If present, the namespace scope for this CLI request. -v, --v int Sets the log level verbosity. Use \"furiko [command] --help\" for more information about a command. Example Usage List all JobConfigs: $ furiko list jobconfigs NAME STATE ACTIVE QUEUED CRON SCHEDULE LAST SCHEDULED daily-cronjob-sends-email-report ReadyEnabled 0 0 15 8 * * * 8h weekly-report-job ReadyEnabled 0 0 0 8 * * MON 2d refund-customer-payments Ready 1 2 Get a single JobConfig: $ furiko get jobconfig weekly-report-job JOB CONFIG METADATA Name: weekly-report-job Namespace: default Created: Tue, 24 May 2022 16 :05:16 +08:00 ( 8 days ago ) CONCURRENCY Policy: Forbid JOB STATUS State: ReadyEnabled Queued Jobs: 0 Active Jobs: 0 Last Scheduled: Mon, 30 May 2022 08 :00:00 +08:00 ( 2 days ago ) Run a JobConfig to create an ad-hoc Job , prompting the user to enter job options : $ furiko run jobconfig-sample","title":"Using the CLI"},{"location":"guide/setup/cli/#using-the-cli","text":"This guide provides details on how to install and use the Furiko CLI tool, furiko .","title":"Using the CLI"},{"location":"guide/setup/cli/#installation-instructions","text":"Currently, furiko is only released via Furiko's GitHub releases. You can install furiko by downloading the executable binary from Furiko's GitHub releases page directly. Download the binary that matches your system's OS and architecture (e.g. furiko_darwin_amd64 ). Rename the binary to furiko and make it executable: chmod +x furiko","title":"Installation Instructions"},{"location":"guide/setup/cli/#usage-instructions","text":"Running furiko --help will list all available commands for the CLI tool. $ furiko --help Command-line utility to manage Furiko. Usage: furiko [ command ] Available Commands: completion generate the autocompletion script for the specified shell disable Disable automatic scheduling for a JobConfig. enable Enable automatic scheduling for a JobConfig. get Get one or more resources by name. help Help about any command kill Kill an ongoing Job. list List all resources by kind. run Run a new Job. Flags: --dynamic-config-name string Overrides the name of the dynamic cluster config. ( default \"execution-dynamic-config\" ) --dynamic-config-namespace string Overrides the namespace of the dynamic cluster config. ( default \"furiko-system\" ) -h, --help help for furiko --kubeconfig string Path to the kubeconfig file to use for CLI requests. -n, --namespace string If present, the namespace scope for this CLI request. -v, --v int Sets the log level verbosity. Use \"furiko [command] --help\" for more information about a command.","title":"Usage Instructions"},{"location":"guide/setup/cli/#example-usage","text":"List all JobConfigs: $ furiko list jobconfigs NAME STATE ACTIVE QUEUED CRON SCHEDULE LAST SCHEDULED daily-cronjob-sends-email-report ReadyEnabled 0 0 15 8 * * * 8h weekly-report-job ReadyEnabled 0 0 0 8 * * MON 2d refund-customer-payments Ready 1 2 Get a single JobConfig: $ furiko get jobconfig weekly-report-job JOB CONFIG METADATA Name: weekly-report-job Namespace: default Created: Tue, 24 May 2022 16 :05:16 +08:00 ( 8 days ago ) CONCURRENCY Policy: Forbid JOB STATUS State: ReadyEnabled Queued Jobs: 0 Active Jobs: 0 Last Scheduled: Mon, 30 May 2022 08 :00:00 +08:00 ( 2 days ago ) Run a JobConfig to create an ad-hoc Job , prompting the user to enter job options : $ furiko run jobconfig-sample","title":"Example Usage"},{"location":"guide/setup/configuration/","text":"Configuring Furiko Furiko allows extensive configuration of its behavior, while also defaulting to recommended, sane defaults out of the box. Editing the ConfigMap The main method of configuring Furiko is via dynamic configuration . As the name suggests, configuration can be dynamically updated and will take effect immediately without requiring a restart of Furiko services. If you used the YAML installation method , a sample ConfigMap has been created for you. Use kubectl to open and edit the configuration as follows: kubectl -n furiko-system edit cm execution-dynamic-config The full description of all dynamic configuration fields can be found in the Reference .","title":"Configuring Furiko"},{"location":"guide/setup/configuration/#configuring-furiko","text":"Furiko allows extensive configuration of its behavior, while also defaulting to recommended, sane defaults out of the box.","title":"Configuring Furiko"},{"location":"guide/setup/configuration/#editing-the-configmap","text":"The main method of configuring Furiko is via dynamic configuration . As the name suggests, configuration can be dynamically updated and will take effect immediately without requiring a restart of Furiko services. If you used the YAML installation method , a sample ConfigMap has been created for you. Use kubectl to open and edit the configuration as follows: kubectl -n furiko-system edit cm execution-dynamic-config The full description of all dynamic configuration fields can be found in the Reference .","title":"Editing the ConfigMap"},{"location":"guide/setup/install/","text":"Installing Furiko This page explains various methods of installation for Furiko. Tip The primary distribution channel for Furiko's releases is on the GitHub Releases page . Do keep an eye on that page for latest updates for Furiko. Prerequisites This guide assumes the following prerequisites: You have a working Kubernetes cluster. Your Kubernetes cluster has a CNI plugin installed, such as Calico. You can connect to the cluster with credentials with sufficiently high privileges to install resources. See Advanced Installation for a detailed description of the resources that need to be installed. From YAML The recommended approach to use the distributed YAML manifests on the GitHub Releases page , and using kubectl to install the manifests provided. The following command installs the Furiko Execution component in a single line: kubectl apply -f https://github.com/furiko-io/furiko/releases/download/v0.2.0/furiko-execution.yaml Refer to the GitHub Releases page for a full list of available releases. The Furiko core components will be installed in the furiko-system namespace. To see all resources that were installed, use kubectl -n furiko-system get all : $ kubectl -n furiko-system get all NAME READY STATUS RESTARTS AGE pod/execution-controller-6d78b46c6c-67j5s 2/2 Running 0 101m pod/execution-webhook-6f66d5f75c-5bjwz 2/2 Running 0 101m pod/execution-webhook-certgen-4pnz5 0/2 Completed 0 9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/execution-webhook-metrics-service ClusterIP 10.96.240.36 <none> 8443/TCP 101m service/execution-webhook-service ClusterIP 10.96.83.234 <none> 443/TCP 101m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/execution-controller 1/1 1 1 101m deployment.apps/execution-webhook 1/1 1 1 101m NAME DESIRED CURRENT READY AGE replicaset.apps/execution-controller-6d78b46c6c 1 1 1 101m replicaset.apps/execution-webhook-6f66d5f75c 1 1 1 101m NAME COMPLETIONS DURATION AGE job.batch/execution-webhook-certgen 1/1 2s 9s We have just installed the execution-controller and execution-webhook Deployments. Once running the cluster can accept and reconcile Execution CRDs. More information can be found in the Architecture section of the documentation. Warning Furiko's container images are currently hosted on Docker Hub. You may encounter rate limits if using a shared IP address, especially in a large organization. Refer to Docker's official guide on rate limiting for more information. Manual Installation For advanced use cases, Furiko also offers manual installation methods. However, it is recommended to use the YAML method of installation and fixing any issues there, instead of performing a full manual installation below. See the guide on Advanced Installation for more details.","title":"Installing Furiko"},{"location":"guide/setup/install/#installing-furiko","text":"This page explains various methods of installation for Furiko. Tip The primary distribution channel for Furiko's releases is on the GitHub Releases page . Do keep an eye on that page for latest updates for Furiko.","title":"Installing Furiko"},{"location":"guide/setup/install/#prerequisites","text":"This guide assumes the following prerequisites: You have a working Kubernetes cluster. Your Kubernetes cluster has a CNI plugin installed, such as Calico. You can connect to the cluster with credentials with sufficiently high privileges to install resources. See Advanced Installation for a detailed description of the resources that need to be installed.","title":"Prerequisites"},{"location":"guide/setup/install/#from-yaml","text":"The recommended approach to use the distributed YAML manifests on the GitHub Releases page , and using kubectl to install the manifests provided. The following command installs the Furiko Execution component in a single line: kubectl apply -f https://github.com/furiko-io/furiko/releases/download/v0.2.0/furiko-execution.yaml Refer to the GitHub Releases page for a full list of available releases. The Furiko core components will be installed in the furiko-system namespace. To see all resources that were installed, use kubectl -n furiko-system get all : $ kubectl -n furiko-system get all NAME READY STATUS RESTARTS AGE pod/execution-controller-6d78b46c6c-67j5s 2/2 Running 0 101m pod/execution-webhook-6f66d5f75c-5bjwz 2/2 Running 0 101m pod/execution-webhook-certgen-4pnz5 0/2 Completed 0 9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/execution-webhook-metrics-service ClusterIP 10.96.240.36 <none> 8443/TCP 101m service/execution-webhook-service ClusterIP 10.96.83.234 <none> 443/TCP 101m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/execution-controller 1/1 1 1 101m deployment.apps/execution-webhook 1/1 1 1 101m NAME DESIRED CURRENT READY AGE replicaset.apps/execution-controller-6d78b46c6c 1 1 1 101m replicaset.apps/execution-webhook-6f66d5f75c 1 1 1 101m NAME COMPLETIONS DURATION AGE job.batch/execution-webhook-certgen 1/1 2s 9s We have just installed the execution-controller and execution-webhook Deployments. Once running the cluster can accept and reconcile Execution CRDs. More information can be found in the Architecture section of the documentation. Warning Furiko's container images are currently hosted on Docker Hub. You may encounter rate limits if using a shared IP address, especially in a large organization. Refer to Docker's official guide on rate limiting for more information.","title":"From YAML"},{"location":"guide/setup/install/#manual-installation","text":"For advanced use cases, Furiko also offers manual installation methods. However, it is recommended to use the YAML method of installation and fixing any issues there, instead of performing a full manual installation below. See the guide on Advanced Installation for more details.","title":"Manual Installation"},{"location":"reference/overview/","text":"Reference This section contains API and configuration references.","title":"Overview"},{"location":"reference/overview/#reference","text":"This section contains API and configuration references.","title":"Reference"},{"location":"reference/configuration/bootstrap/","text":"Bootstrap Configuration This page lists the bootstrap configuration that is common to all components in Furiko. Component-specific configuration can be found in other children pages in the Configuration section. Configuration Options There are no default values for bootstrap configuration options (for the most part), so every field must be specified. defaultResync Specifies the default resync duration for all informers. Accepts a Golang duration string (e.g. 10m ). defaultResync : 10m dynamicConfigs Specifies how to load the Dynamic Configuration . # Default value differs depending the actual component. dynamicConfigs : configMap : namespace : furiko-system name : execution-dynamic-config secret : namespace : furiko-system name : execution-dynamic-config http Specifies how to set up HTTP handlers, including Prometheus metrics, readiness and liveness probes. All HTTP endpoints are currently served on a single HTTP endpoint, as such there is only one bindAddress specified. http : bindAddress : \":8080\" metrics : enabled : true health : enabled : true readinessProbePath : \"/readyz\" livenessProbePath : \"/healthz\" leaderElection Specifies whether and how to set up leader election. Applies only to Controller components (i.e. does not apply to Webhook components). leaderElection : enabled : true leaseName : execution-controller leaseNamespace : furiko-system leaseDuration : 30s renewDeadline : 15s retryPeriod : 5s controllerConcurrency Specifies the concurrency values for individual controllers. Applies only to Controller components (i.e. does not apply to Webhook components). Tip Concurrency can be specified either as an absolute number of worker goroutines (using workers ), or as a factor of the number of CPUs allocated to the container (using factorOfCPUs ). Using factorOfCPUs allows you to scale a controller vertically by changing a single CPU limit knob, rather than having to individually tune each knob when increasing the resources allocated to the container. This is useful because most controllers should be CPU-bound. controllerConcurrency : <controller name> : factorOfCPUs : 4","title":"Bootstrap Config"},{"location":"reference/configuration/bootstrap/#bootstrap-configuration","text":"This page lists the bootstrap configuration that is common to all components in Furiko. Component-specific configuration can be found in other children pages in the Configuration section.","title":"Bootstrap Configuration"},{"location":"reference/configuration/bootstrap/#configuration-options","text":"There are no default values for bootstrap configuration options (for the most part), so every field must be specified.","title":"Configuration Options"},{"location":"reference/configuration/bootstrap/#defaultresync","text":"Specifies the default resync duration for all informers. Accepts a Golang duration string (e.g. 10m ). defaultResync : 10m","title":"defaultResync"},{"location":"reference/configuration/bootstrap/#dynamicconfigs","text":"Specifies how to load the Dynamic Configuration . # Default value differs depending the actual component. dynamicConfigs : configMap : namespace : furiko-system name : execution-dynamic-config secret : namespace : furiko-system name : execution-dynamic-config","title":"dynamicConfigs"},{"location":"reference/configuration/bootstrap/#http","text":"Specifies how to set up HTTP handlers, including Prometheus metrics, readiness and liveness probes. All HTTP endpoints are currently served on a single HTTP endpoint, as such there is only one bindAddress specified. http : bindAddress : \":8080\" metrics : enabled : true health : enabled : true readinessProbePath : \"/readyz\" livenessProbePath : \"/healthz\"","title":"http"},{"location":"reference/configuration/bootstrap/#leaderelection","text":"Specifies whether and how to set up leader election. Applies only to Controller components (i.e. does not apply to Webhook components). leaderElection : enabled : true leaseName : execution-controller leaseNamespace : furiko-system leaseDuration : 30s renewDeadline : 15s retryPeriod : 5s","title":"leaderElection"},{"location":"reference/configuration/bootstrap/#controllerconcurrency","text":"Specifies the concurrency values for individual controllers. Applies only to Controller components (i.e. does not apply to Webhook components). Tip Concurrency can be specified either as an absolute number of worker goroutines (using workers ), or as a factor of the number of CPUs allocated to the container (using factorOfCPUs ). Using factorOfCPUs allows you to scale a controller vertically by changing a single CPU limit knob, rather than having to individually tune each knob when increasing the resources allocated to the container. This is useful because most controllers should be CPU-bound. controllerConcurrency : <controller name> : factorOfCPUs : 4","title":"controllerConcurrency"},{"location":"reference/configuration/dynamic/","text":"Dynamic Configuration This page introduces how to use Dynamic Configuration in Furiko. Overview As compared to bootstrap configuration , dynamic configuration can be updated and will take effect immediately upon saving. This type of configuration typically contains knobs to tune the behavior of Furiko, so that any configuration does not require a full restart of any of Furiko services running in the cluster. Data Source Dynamic configuration is typically stored in a ConfigMap, such as execution-dynamic-config in furiko-system namespace. It can also be stored in a Secret instead if sensitive values need to be stored in a more secure fashion. The controller and webhook services need to know how to load the dynamic configuration, and thus the references to the dynamic configuration is stored in the bootstrap configuration. If the dynamic configuration is not found, it will fallback to default values. Example Bootstrap Config # The following bootstrap config describes to ExecutionController # how to load the dynamic config. apiVersion : config.furiko.io/v1 kind : ExecutionControllerConfig dynamicConfigs : configMap : namespace : furiko-system name : execution-dynamic-config See Bootstrap Config for more details. Config Data Dynamic configuration is split into several configuration files, which correspond to the keys in the ConfigMap itself. More information can be found in the Dynamic Config pages under each component. The following is an example of how a dynamic configuration ConfigMap may look like: Example Dynamic ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : execution-dynamic-config namespace : furiko-system # Each key in the ConfigMap corresponds to a single dynamic config name. data : # Data in the config must be a YAML or JSON-encoded string. jobs : | defaultTTLSecondsAfterFinished: 3600 defaultPendingTimeoutSeconds: 900 cron : | cronFormat: \"standard\" cronHashNames: true Configuration Options Documentation on Furiko dynamic configuration is split on a per-component basis. The following lists the configuration documentation for each component. Execution Dynamic Config","title":"Dynamic Config"},{"location":"reference/configuration/dynamic/#dynamic-configuration","text":"This page introduces how to use Dynamic Configuration in Furiko.","title":"Dynamic Configuration"},{"location":"reference/configuration/dynamic/#overview","text":"As compared to bootstrap configuration , dynamic configuration can be updated and will take effect immediately upon saving. This type of configuration typically contains knobs to tune the behavior of Furiko, so that any configuration does not require a full restart of any of Furiko services running in the cluster.","title":"Overview"},{"location":"reference/configuration/dynamic/#data-source","text":"Dynamic configuration is typically stored in a ConfigMap, such as execution-dynamic-config in furiko-system namespace. It can also be stored in a Secret instead if sensitive values need to be stored in a more secure fashion. The controller and webhook services need to know how to load the dynamic configuration, and thus the references to the dynamic configuration is stored in the bootstrap configuration. If the dynamic configuration is not found, it will fallback to default values. Example Bootstrap Config # The following bootstrap config describes to ExecutionController # how to load the dynamic config. apiVersion : config.furiko.io/v1 kind : ExecutionControllerConfig dynamicConfigs : configMap : namespace : furiko-system name : execution-dynamic-config See Bootstrap Config for more details.","title":"Data Source"},{"location":"reference/configuration/dynamic/#config-data","text":"Dynamic configuration is split into several configuration files, which correspond to the keys in the ConfigMap itself. More information can be found in the Dynamic Config pages under each component. The following is an example of how a dynamic configuration ConfigMap may look like: Example Dynamic ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : execution-dynamic-config namespace : furiko-system # Each key in the ConfigMap corresponds to a single dynamic config name. data : # Data in the config must be a YAML or JSON-encoded string. jobs : | defaultTTLSecondsAfterFinished: 3600 defaultPendingTimeoutSeconds: 900 cron : | cronFormat: \"standard\" cronHashNames: true","title":"Config Data"},{"location":"reference/configuration/dynamic/#configuration-options","text":"Documentation on Furiko dynamic configuration is split on a per-component basis. The following lists the configuration documentation for each component. Execution Dynamic Config","title":"Configuration Options"},{"location":"reference/configuration/overview/","text":"Configuration Configuration of Furiko is split into two main categories: bootstrap configuration and dynamic configuration . Bootstrap Configuration Bootstrap configuration refers to configuration fields that are specified when starting a component, such as a controller or a webhook server. This configuration typically contains information about port numbers to listen on, whether leader election is enabled, and so on. Modifying these configuration values require a restart of the component to take effect. The bootstrap configuration is almost identical for almost all components. More details can be found in Bootstrap Config . Dynamic Configuration Dynamic configuration , on the other hand, refers to configuration that is stored in some remote source, typically a ConfigMap, which will take effect immediately upon updating it. More details can be found in Dynamic Config .","title":"Overview"},{"location":"reference/configuration/overview/#configuration","text":"Configuration of Furiko is split into two main categories: bootstrap configuration and dynamic configuration .","title":"Configuration"},{"location":"reference/configuration/overview/#bootstrap-configuration","text":"Bootstrap configuration refers to configuration fields that are specified when starting a component, such as a controller or a webhook server. This configuration typically contains information about port numbers to listen on, whether leader election is enabled, and so on. Modifying these configuration values require a restart of the component to take effect. The bootstrap configuration is almost identical for almost all components. More details can be found in Bootstrap Config .","title":"Bootstrap Configuration"},{"location":"reference/configuration/overview/#dynamic-configuration","text":"Dynamic configuration , on the other hand, refers to configuration that is stored in some remote source, typically a ConfigMap, which will take effect immediately upon updating it. More details can be found in Dynamic Config .","title":"Dynamic Configuration"},{"location":"reference/configuration/execution/","text":"Execution Configuration This section contains the full configuration for all Execution components. Controller Bootstrap Config Webhook Bootstrap Config Dynamic Config","title":"Execution"},{"location":"reference/configuration/execution/#execution-configuration","text":"This section contains the full configuration for all Execution components. Controller Bootstrap Config Webhook Bootstrap Config Dynamic Config","title":"Execution Configuration"},{"location":"reference/configuration/execution/controller-config/","text":"Execution Controller Config This page contains the full bootstrap configuration for ExecutionController. # Here we define the bootstrap config for execution-controller. # These values are used for bootstrapping of the controller manager at startup, # so changing any values here require a restart of execution-controller in order # to take effect. apiVersion : config.furiko.io/v1 kind : ExecutionControllerConfig # defaultResync controls the default resync duration. defaultResync : 10m # leaderElection controls leader election configuration. leaderElection : # enabled controls whether leader election is enabled. enabled : true # leaseName controls the name used for the lease. # If left empty, then a default name will be used. leaseName : execution-controller # leaseNamespace controls the namespace used for the lease. leaseNamespace : furiko-system # leaseDuration is the duration that non-leader candidates will wait after # observing a leadership renewal until attempting to acquire leadership of a # led but unrenewed leader slot. This is effectively the maximum duration that # a leader can be stopped before it is replaced by another candidate. This is # only applicable if leader election is enabled. leaseDuration : 30s # renewDeadline is the interval between attempts by the acting master to renew # a leadership slot before it stops leading. This must be less than or equal to # the lease duration. This is only applicable if leader election is enabled. renewDeadline : 15s # retryPeriod is the duration the clients should wait between attempting # acquisition and renewal of a leadership. This is only applicable if leader # election is enabled. retryPeriod : 5s # dynamicConfigs defines how to load dynamic configs. dynamicConfigs : # configMap defines how the dynamic ConfigMap is loaded. configMap : namespace : furiko-system name : execution-dynamic-config # secret defines how the dynamic Secret is loaded. secret : namespace : furiko-system name : execution-dynamic-config # HTTP handler configuration. http : # bindAddress is the TCP address that the controller should bind to for serving # HTTP requests. bindAddress : \":8080\" # metrics controls metrics serving. metrics : # enabled is whether the controller manager enables serving Prometheus metrics. enabled : true # health controls health status serving. health : # enabled is whether the controller manager enables serving health probes. enabled : true # readinessProbePath is the path to the readiness probe. readinessProbePath : \"/readyz\" # livenessProbePath is the path to the liveness probe. livenessProbePath : \"/healthz\" # controllerConcurrency defines the concurrency factor for individual controllers. controllerConcurrency : # cron controls the concurrency for the Cron controller. cron : factorOfCPUs : 4 # job controls the concurrency for the Job controller. job : factorOfCPUs : 4 # jobConfig controls the concurrency for the JobConfig controller. jobConfig : factorOfCPUs : 4 # jobQueue controls the concurrency for the JobQueue controller. jobQueue : factorOfCPUs : 4","title":"Controller Config"},{"location":"reference/configuration/execution/controller-config/#execution-controller-config","text":"This page contains the full bootstrap configuration for ExecutionController. # Here we define the bootstrap config for execution-controller. # These values are used for bootstrapping of the controller manager at startup, # so changing any values here require a restart of execution-controller in order # to take effect. apiVersion : config.furiko.io/v1 kind : ExecutionControllerConfig # defaultResync controls the default resync duration. defaultResync : 10m # leaderElection controls leader election configuration. leaderElection : # enabled controls whether leader election is enabled. enabled : true # leaseName controls the name used for the lease. # If left empty, then a default name will be used. leaseName : execution-controller # leaseNamespace controls the namespace used for the lease. leaseNamespace : furiko-system # leaseDuration is the duration that non-leader candidates will wait after # observing a leadership renewal until attempting to acquire leadership of a # led but unrenewed leader slot. This is effectively the maximum duration that # a leader can be stopped before it is replaced by another candidate. This is # only applicable if leader election is enabled. leaseDuration : 30s # renewDeadline is the interval between attempts by the acting master to renew # a leadership slot before it stops leading. This must be less than or equal to # the lease duration. This is only applicable if leader election is enabled. renewDeadline : 15s # retryPeriod is the duration the clients should wait between attempting # acquisition and renewal of a leadership. This is only applicable if leader # election is enabled. retryPeriod : 5s # dynamicConfigs defines how to load dynamic configs. dynamicConfigs : # configMap defines how the dynamic ConfigMap is loaded. configMap : namespace : furiko-system name : execution-dynamic-config # secret defines how the dynamic Secret is loaded. secret : namespace : furiko-system name : execution-dynamic-config # HTTP handler configuration. http : # bindAddress is the TCP address that the controller should bind to for serving # HTTP requests. bindAddress : \":8080\" # metrics controls metrics serving. metrics : # enabled is whether the controller manager enables serving Prometheus metrics. enabled : true # health controls health status serving. health : # enabled is whether the controller manager enables serving health probes. enabled : true # readinessProbePath is the path to the readiness probe. readinessProbePath : \"/readyz\" # livenessProbePath is the path to the liveness probe. livenessProbePath : \"/healthz\" # controllerConcurrency defines the concurrency factor for individual controllers. controllerConcurrency : # cron controls the concurrency for the Cron controller. cron : factorOfCPUs : 4 # job controls the concurrency for the Job controller. job : factorOfCPUs : 4 # jobConfig controls the concurrency for the JobConfig controller. jobConfig : factorOfCPUs : 4 # jobQueue controls the concurrency for the JobQueue controller. jobQueue : factorOfCPUs : 4","title":"Execution Controller Config"},{"location":"reference/configuration/execution/controller-flags/","text":"Execution Controller Flags This page contains the full command-line flags for ExecutionController. Command-Line Flags --config=<path> Defines the path to the bootstrap configuration file , either in JSON or YAML. --teardown-timeout=2m Defines the timeout to wait for downstream workers and controllers to gracefully shut down, before forcibly quitting, when a termination signal ( SIGINT , SIGTERM ) is received.","title":"Controller Flags"},{"location":"reference/configuration/execution/controller-flags/#execution-controller-flags","text":"This page contains the full command-line flags for ExecutionController.","title":"Execution Controller Flags"},{"location":"reference/configuration/execution/controller-flags/#command-line-flags","text":"--config=<path> Defines the path to the bootstrap configuration file , either in JSON or YAML. --teardown-timeout=2m Defines the timeout to wait for downstream workers and controllers to gracefully shut down, before forcibly quitting, when a termination signal ( SIGINT , SIGTERM ) is received.","title":"Command-Line Flags"},{"location":"reference/configuration/execution/dynamic-config/","text":"Execution Dynamic Config This page documents the dynamic configuration for all execution components. Configuration Options Each of the following sections corresponds to a single key in the dynamic configuration ConfigMap. jobs Defines configuration for Jobs . defaultTTLSecondsAfterFinished Defines the default value of ttlSecondsAfterFinished for a Job if it is not defined. For more information, see Garbage Collection . To avoid issues with huge amounts of finished Jobs building up, a non-zero TTL is enforced. Defaults to 3600 (1 hour). defaultPendingTimeoutSeconds Defines the default value of pendingTimeoutSeconds for a Job if it is not defined. For more information, see Pending Timeouts . Defaults to 900 (15 minutes). forceDeleteTaskTimeoutSeconds The duration before the controller uses force deletion instead of normal deletion. This timeout is computed from the deletionTimestamp of the object, which may also include an additional delay of deletionGracePeriodSeconds . For more information, see Force Deletion . Force deletion causes the task to be deleted without confirmation that the task has already terminated. When pod is used for taskTemplate, this means that the container may remain running on the node even though the task or Job is already marked as terminated. Set this value to 0 to disable force deletion globally. Individual jobs can also specify spec.forbidTaskForceDeletion in the JobTemplate to disable force deletion if this behavior is not desired. Defaults to 900 (15 minutes). jobConfigs Defines configuration for JobConfigs . maxEnqueuedJobs The global maximum enqueued jobs that can be enqueued for a single JobConfig. Defaults to 20 . cron Defines configuration for parsing cron expressions within the cluster. cronFormat Specifies the format used to parse cron expressions. The only difference is in the Day of Week digit parsing . Only the following values are supported: standard (default): Standard cron format. quartz : Quartz cron format. cronHashNames Specifies if cron expressions should be hashed using the JobConfig's name. Enabling this option would allow H tokens to be used . If disabled, any JobConfigs that use the H syntax will throw a parse error. Defaults to true . cronHashSecondsByDefault Specifies if the seconds field of a cron expression should be a H or 0 by default if omitted. If enabled, the default seconds field will be H , otherwise it will be 0 . For JobConfigs which use a short cron expression format (i.e. 5 or 6 tokens long), the seconds field is omitted and is typically assumed to be 0 (e.g. 5 10 * * * means to run at 10:05:00 every day). Enabling this option will allow JobConfigs to be scheduled across the minute, improving load balancing. Users can still choose to start at 0 seconds by explicitly specifying a long cron expression format with 0 in the seconds field. In the above example, this would be 0 5 10 * * * * . Defaults to false . cronHashFields Specifies if the \"type of field\" should be hashed along with the JobConfig's name. For example, H H * * * * * will always hash the seconds and minutes to the same value, for example 00:37:37, 01:37:37, etc. Enabling this option will append additional keys to be hashed to introduce additional non-determinism. Defaults to true . defaultTimezone Defines a default timezone to use for JobConfigs that do not specify a timezone . Defaults to UTC . maxMissedSchedules Defines a maximum number of jobs that the controller should back-schedule, or attempt to create after coming back up from downtime. Having a sane value here would prevent a thundering herd of jobs being scheduled that would exhaust resources in the cluster. For more information, see Back-Scheduling . In practice, setting this to too high of a value could result in accidental resource exhaustion in the cluster if the controller was intentionally shut down for an extended period of time. Set this to 0 to disable back-scheduling entirely. Defaults to 5. maxDowntimeThresholdSeconds Defines the maximum downtime that the controller can tolerate. If the controller was intentionally shut down for an extended period of time, we should not attempt to back-schedule jobs once it was started. For more information, see Back-Scheduling . In practice, setting this to too high of a value means that jobs could be ridiculously delayed when they are better off being skipped entirely (say, sending out a end-of-week report on the following Monday instead). Defaults to 300 (5 minutes). It is recommended to tune this to the maximum realistic outage duration of the controller. Sample Configuration The following sample shows how to configure the full dynamic configuration ConfigMap of the execution component, as well as the default configuration values for each field. Full Sample Configuration apiVersion : v1 kind : ConfigMap metadata : name : execution-dynamic-config namespace : furiko-system data : _readme : | # This ConfigMap contains the dynamic config for execution-controller. # We can tune several knobs in execution-controller without requiring a restart. # Each file in this ConfigMap groups together configuration of a single sub-component. # As a start, we have populated a set of sane default values for you. # More info: https://furiko.io/reference/configuration/dynamic/ jobs : | apiVersion: config.furiko.io/v1alpha1 kind: JobExecutionConfig # The default time-to-live (TTL) for a Job after it has finished. Lower this # value to reduce the strain on the cluster/kubelet. Set to 0 to delete immediately # after the Job is finished. defaultTTLSecondsAfterFinished: 3600 # The default timeout for a task to remain in a pending state. Defaults to 15 minutes # in order to prevent jobs from retrying indefinitely. # # To prevent setting a default pending timeout globally, set this to 0. Individual jobs # can still specify spec.taskPendingTimeoutSeconds in the JobTemplate to override this # global default value. defaultPendingTimeoutSeconds: 900 # The duration before the controller uses force deletion instead of normal deletion. # This timeout is computed from the deletionTimestamp of the object, which may also include # an additional delay of deletionGracePeriodSeconds. # # Force deletion causes the task to be deleted without confirmation that the task has already # terminated. When pod is used for taskTemplate, this means that # # Set this value to 0 to disable force deletion globally. Individual jobs can also specify # spec.forbidTaskForceDeletion in the JobTemplate to disable force deletion if this # behavior is not desired. forceDeleteTaskTimeoutSeconds: 900 jobConfigs : | apiVersion: config.furiko.io/v1alpha1 kind: JobConfigExecutionConfig # The global maximum enqueued jobs that can be enqueued for a single JobConfig. maxEnqueuedJobs: 20 cron : | apiVersion: config.furiko.io/v1alpha1 kind: CronExecutionConfig # Specifies the format used to parse cron expressions. Select between \"standard\" # (default) or \"quartz\". cronFormat: \"standard\" # Specifies if cron expressions should be hashed using the JobConfig's name. # # This enables \"hash cron expressions\", which looks like `0 H * * *`. This # particular example means to run once a day on the 0th minute of some hour, # which will be determined by hashing the JobConfig's name. By enabling this # option, JobConfigs that use such cron schedules will be load balanced across # the cluster. # # If disabled, any JobConfigs that use the `H` syntax will throw a parse error. cronHashNames: true # Specifies if the seconds field of a cron expression should be a `H` or `0` # by default. If enabled, it will be `H`, otherwise it will default to `0`. # # For JobConfigs which use a short cron expression format (i.e. 5 or 6 tokens # long), the seconds field is omitted and is typically assumed to be `0` (e.g. # `5 10 * * *` means to run at 10:05:00 every day). Enabling this option will # allow JobConfigs to be scheduled across the minute, improving load balancing. # # Users can still choose to start at 0 seconds by explicitly specifying a long # cron expression format with `0` in the seconds field. In the above example, # this would be `0 5 10 * * * *`. cronHashSecondsByDefault: false # Specifies if the fields should be hashed along with the JobConfig's name. # # For example, `H H * * * * *` will always hash the seconds and minutes to the # same value, for example 00:37:37, 01:37:37, etc. Enabling this option will # append additional keys to be hashed to introduce additional non-determinism. cronHashFields: true # Defines a default timezone to use for JobConfigs that do not specify a timezone. # If left empty, UTC will be used as the default timezone. defaultTimezone: \"UTC\" # Defines the maximum number of jobs that the controller should back-schedule, # or attempt to create after coming back up from downtime. Having a sane value # here would prevent a thundering herd of jobs being scheduled that would exhaust # resources in the cluster. # # Set this to 0 to disable back-scheduling. maxMissedSchedules: 5 # Defines the maximum downtime that the controller can tolerate. If the controller # was shut down for an extended period of time, any jobs that should have been created # beyond the maximum downtime threshold would not be back-scheduled once it is started again. maxDowntimeThresholdSeconds: 300","title":"Dynamic Config"},{"location":"reference/configuration/execution/dynamic-config/#execution-dynamic-config","text":"This page documents the dynamic configuration for all execution components.","title":"Execution Dynamic Config"},{"location":"reference/configuration/execution/dynamic-config/#configuration-options","text":"Each of the following sections corresponds to a single key in the dynamic configuration ConfigMap.","title":"Configuration Options"},{"location":"reference/configuration/execution/dynamic-config/#jobs","text":"Defines configuration for Jobs .","title":"jobs"},{"location":"reference/configuration/execution/dynamic-config/#defaultttlsecondsafterfinished","text":"Defines the default value of ttlSecondsAfterFinished for a Job if it is not defined. For more information, see Garbage Collection . To avoid issues with huge amounts of finished Jobs building up, a non-zero TTL is enforced. Defaults to 3600 (1 hour).","title":"defaultTTLSecondsAfterFinished"},{"location":"reference/configuration/execution/dynamic-config/#defaultpendingtimeoutseconds","text":"Defines the default value of pendingTimeoutSeconds for a Job if it is not defined. For more information, see Pending Timeouts . Defaults to 900 (15 minutes).","title":"defaultPendingTimeoutSeconds"},{"location":"reference/configuration/execution/dynamic-config/#forcedeletetasktimeoutseconds","text":"The duration before the controller uses force deletion instead of normal deletion. This timeout is computed from the deletionTimestamp of the object, which may also include an additional delay of deletionGracePeriodSeconds . For more information, see Force Deletion . Force deletion causes the task to be deleted without confirmation that the task has already terminated. When pod is used for taskTemplate, this means that the container may remain running on the node even though the task or Job is already marked as terminated. Set this value to 0 to disable force deletion globally. Individual jobs can also specify spec.forbidTaskForceDeletion in the JobTemplate to disable force deletion if this behavior is not desired. Defaults to 900 (15 minutes).","title":"forceDeleteTaskTimeoutSeconds"},{"location":"reference/configuration/execution/dynamic-config/#jobconfigs","text":"Defines configuration for JobConfigs .","title":"jobConfigs"},{"location":"reference/configuration/execution/dynamic-config/#maxenqueuedjobs","text":"The global maximum enqueued jobs that can be enqueued for a single JobConfig. Defaults to 20 .","title":"maxEnqueuedJobs"},{"location":"reference/configuration/execution/dynamic-config/#cron","text":"Defines configuration for parsing cron expressions within the cluster.","title":"cron"},{"location":"reference/configuration/execution/dynamic-config/#cronformat","text":"Specifies the format used to parse cron expressions. The only difference is in the Day of Week digit parsing . Only the following values are supported: standard (default): Standard cron format. quartz : Quartz cron format.","title":"cronFormat"},{"location":"reference/configuration/execution/dynamic-config/#cronhashnames","text":"Specifies if cron expressions should be hashed using the JobConfig's name. Enabling this option would allow H tokens to be used . If disabled, any JobConfigs that use the H syntax will throw a parse error. Defaults to true .","title":"cronHashNames"},{"location":"reference/configuration/execution/dynamic-config/#cronhashsecondsbydefault","text":"Specifies if the seconds field of a cron expression should be a H or 0 by default if omitted. If enabled, the default seconds field will be H , otherwise it will be 0 . For JobConfigs which use a short cron expression format (i.e. 5 or 6 tokens long), the seconds field is omitted and is typically assumed to be 0 (e.g. 5 10 * * * means to run at 10:05:00 every day). Enabling this option will allow JobConfigs to be scheduled across the minute, improving load balancing. Users can still choose to start at 0 seconds by explicitly specifying a long cron expression format with 0 in the seconds field. In the above example, this would be 0 5 10 * * * * . Defaults to false .","title":"cronHashSecondsByDefault"},{"location":"reference/configuration/execution/dynamic-config/#cronhashfields","text":"Specifies if the \"type of field\" should be hashed along with the JobConfig's name. For example, H H * * * * * will always hash the seconds and minutes to the same value, for example 00:37:37, 01:37:37, etc. Enabling this option will append additional keys to be hashed to introduce additional non-determinism. Defaults to true .","title":"cronHashFields"},{"location":"reference/configuration/execution/dynamic-config/#defaulttimezone","text":"Defines a default timezone to use for JobConfigs that do not specify a timezone . Defaults to UTC .","title":"defaultTimezone"},{"location":"reference/configuration/execution/dynamic-config/#maxmissedschedules","text":"Defines a maximum number of jobs that the controller should back-schedule, or attempt to create after coming back up from downtime. Having a sane value here would prevent a thundering herd of jobs being scheduled that would exhaust resources in the cluster. For more information, see Back-Scheduling . In practice, setting this to too high of a value could result in accidental resource exhaustion in the cluster if the controller was intentionally shut down for an extended period of time. Set this to 0 to disable back-scheduling entirely. Defaults to 5.","title":"maxMissedSchedules"},{"location":"reference/configuration/execution/dynamic-config/#maxdowntimethresholdseconds","text":"Defines the maximum downtime that the controller can tolerate. If the controller was intentionally shut down for an extended period of time, we should not attempt to back-schedule jobs once it was started. For more information, see Back-Scheduling . In practice, setting this to too high of a value means that jobs could be ridiculously delayed when they are better off being skipped entirely (say, sending out a end-of-week report on the following Monday instead). Defaults to 300 (5 minutes). It is recommended to tune this to the maximum realistic outage duration of the controller.","title":"maxDowntimeThresholdSeconds"},{"location":"reference/configuration/execution/dynamic-config/#sample-configuration","text":"The following sample shows how to configure the full dynamic configuration ConfigMap of the execution component, as well as the default configuration values for each field. Full Sample Configuration apiVersion : v1 kind : ConfigMap metadata : name : execution-dynamic-config namespace : furiko-system data : _readme : | # This ConfigMap contains the dynamic config for execution-controller. # We can tune several knobs in execution-controller without requiring a restart. # Each file in this ConfigMap groups together configuration of a single sub-component. # As a start, we have populated a set of sane default values for you. # More info: https://furiko.io/reference/configuration/dynamic/ jobs : | apiVersion: config.furiko.io/v1alpha1 kind: JobExecutionConfig # The default time-to-live (TTL) for a Job after it has finished. Lower this # value to reduce the strain on the cluster/kubelet. Set to 0 to delete immediately # after the Job is finished. defaultTTLSecondsAfterFinished: 3600 # The default timeout for a task to remain in a pending state. Defaults to 15 minutes # in order to prevent jobs from retrying indefinitely. # # To prevent setting a default pending timeout globally, set this to 0. Individual jobs # can still specify spec.taskPendingTimeoutSeconds in the JobTemplate to override this # global default value. defaultPendingTimeoutSeconds: 900 # The duration before the controller uses force deletion instead of normal deletion. # This timeout is computed from the deletionTimestamp of the object, which may also include # an additional delay of deletionGracePeriodSeconds. # # Force deletion causes the task to be deleted without confirmation that the task has already # terminated. When pod is used for taskTemplate, this means that # # Set this value to 0 to disable force deletion globally. Individual jobs can also specify # spec.forbidTaskForceDeletion in the JobTemplate to disable force deletion if this # behavior is not desired. forceDeleteTaskTimeoutSeconds: 900 jobConfigs : | apiVersion: config.furiko.io/v1alpha1 kind: JobConfigExecutionConfig # The global maximum enqueued jobs that can be enqueued for a single JobConfig. maxEnqueuedJobs: 20 cron : | apiVersion: config.furiko.io/v1alpha1 kind: CronExecutionConfig # Specifies the format used to parse cron expressions. Select between \"standard\" # (default) or \"quartz\". cronFormat: \"standard\" # Specifies if cron expressions should be hashed using the JobConfig's name. # # This enables \"hash cron expressions\", which looks like `0 H * * *`. This # particular example means to run once a day on the 0th minute of some hour, # which will be determined by hashing the JobConfig's name. By enabling this # option, JobConfigs that use such cron schedules will be load balanced across # the cluster. # # If disabled, any JobConfigs that use the `H` syntax will throw a parse error. cronHashNames: true # Specifies if the seconds field of a cron expression should be a `H` or `0` # by default. If enabled, it will be `H`, otherwise it will default to `0`. # # For JobConfigs which use a short cron expression format (i.e. 5 or 6 tokens # long), the seconds field is omitted and is typically assumed to be `0` (e.g. # `5 10 * * *` means to run at 10:05:00 every day). Enabling this option will # allow JobConfigs to be scheduled across the minute, improving load balancing. # # Users can still choose to start at 0 seconds by explicitly specifying a long # cron expression format with `0` in the seconds field. In the above example, # this would be `0 5 10 * * * *`. cronHashSecondsByDefault: false # Specifies if the fields should be hashed along with the JobConfig's name. # # For example, `H H * * * * *` will always hash the seconds and minutes to the # same value, for example 00:37:37, 01:37:37, etc. Enabling this option will # append additional keys to be hashed to introduce additional non-determinism. cronHashFields: true # Defines a default timezone to use for JobConfigs that do not specify a timezone. # If left empty, UTC will be used as the default timezone. defaultTimezone: \"UTC\" # Defines the maximum number of jobs that the controller should back-schedule, # or attempt to create after coming back up from downtime. Having a sane value # here would prevent a thundering herd of jobs being scheduled that would exhaust # resources in the cluster. # # Set this to 0 to disable back-scheduling. maxMissedSchedules: 5 # Defines the maximum downtime that the controller can tolerate. If the controller # was shut down for an extended period of time, any jobs that should have been created # beyond the maximum downtime threshold would not be back-scheduled once it is started again. maxDowntimeThresholdSeconds: 300","title":"Sample Configuration"},{"location":"reference/configuration/execution/webhook-config/","text":"Execution Webhook Config This page contains the full bootstrap configuration for ExecutionWebhook. # Here we define the bootstrap config for execution-webhook. # These values are used for bootstrapping of the webhook server at startup, # so changing any values here require a restart of execution-webhook in order # to take effect. apiVersion : config.furiko.io/v1 kind : ExecutionWebhookConfig # defaultResync controls the default resync duration. defaultResync : 10m # Webhook server configuration. webhooks : # bindAddress is the TCP address that the controller manager should bind to for # serving webhook requests over HTTPS. bindAddress : \":9443\" # tlsCertFile is the path to the X.509 certificate to use for serving webhook # requests over HTTPS. tlsCertFile : \"/etc/webhook/certs/cert\" # tlsPrivateKeyFile is the path to the private key which corresponds to # TLSCertFile, to use for serving webhook requests over HTTPS. tlsPrivateKeyFile : \"/etc/webhook/certs/key\" # dynamicConfigs defines how to load dynamic configs. dynamicConfigs : # configMap defines how the dynamic ConfigMap is loaded. configMap : namespace : furiko-system name : execution-dynamic-config # secret defines how the dynamic Secret is loaded. secret : namespace : furiko-system name : execution-dynamic-config # HTTP handler configuration. http : # bindAddress is the TCP address that the controller should bind to for serving # HTTP requests. bindAddress : \":8080\" # metrics controls metrics serving. metrics : # enabled is whether the controller manager enables serving Prometheus metrics. enabled : true # health controls health status serving. health : # enabled is whether the controller manager enables serving health probes. enabled : true # readinessProbePath is the path to the readiness probe. readinessProbePath : \"/readyz\" # livenessProbePath is the path to the liveness probe. livenessProbePath : \"/healthz\"","title":"Webhook Config"},{"location":"reference/configuration/execution/webhook-config/#execution-webhook-config","text":"This page contains the full bootstrap configuration for ExecutionWebhook. # Here we define the bootstrap config for execution-webhook. # These values are used for bootstrapping of the webhook server at startup, # so changing any values here require a restart of execution-webhook in order # to take effect. apiVersion : config.furiko.io/v1 kind : ExecutionWebhookConfig # defaultResync controls the default resync duration. defaultResync : 10m # Webhook server configuration. webhooks : # bindAddress is the TCP address that the controller manager should bind to for # serving webhook requests over HTTPS. bindAddress : \":9443\" # tlsCertFile is the path to the X.509 certificate to use for serving webhook # requests over HTTPS. tlsCertFile : \"/etc/webhook/certs/cert\" # tlsPrivateKeyFile is the path to the private key which corresponds to # TLSCertFile, to use for serving webhook requests over HTTPS. tlsPrivateKeyFile : \"/etc/webhook/certs/key\" # dynamicConfigs defines how to load dynamic configs. dynamicConfigs : # configMap defines how the dynamic ConfigMap is loaded. configMap : namespace : furiko-system name : execution-dynamic-config # secret defines how the dynamic Secret is loaded. secret : namespace : furiko-system name : execution-dynamic-config # HTTP handler configuration. http : # bindAddress is the TCP address that the controller should bind to for serving # HTTP requests. bindAddress : \":8080\" # metrics controls metrics serving. metrics : # enabled is whether the controller manager enables serving Prometheus metrics. enabled : true # health controls health status serving. health : # enabled is whether the controller manager enables serving health probes. enabled : true # readinessProbePath is the path to the readiness probe. readinessProbePath : \"/readyz\" # livenessProbePath is the path to the liveness probe. livenessProbePath : \"/healthz\"","title":"Execution Webhook Config"},{"location":"reference/configuration/execution/webhook-flags/","text":"Execution Webhook Flags This page contains the full command-line flags for ExecutionWebhook. Command-Line Flags --config=<path> Defines the path to the bootstrap configuration file , either in JSON or YAML. --teardown-timeout=2m Defines the timeout to wait for downstream workers and webhooks to gracefully shut down, before forcibly quitting, when a termination signal ( SIGINT , SIGTERM ) is received.","title":"Webhook Flags"},{"location":"reference/configuration/execution/webhook-flags/#execution-webhook-flags","text":"This page contains the full command-line flags for ExecutionWebhook.","title":"Execution Webhook Flags"},{"location":"reference/configuration/execution/webhook-flags/#command-line-flags","text":"--config=<path> Defines the path to the bootstrap configuration file , either in JSON or YAML. --teardown-timeout=2m Defines the timeout to wait for downstream workers and webhooks to gracefully shut down, before forcibly quitting, when a termination signal ( SIGINT , SIGTERM ) is received.","title":"Command-Line Flags"}]}